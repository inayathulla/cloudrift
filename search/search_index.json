{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Cloudrift","text":"<p>Pre-apply Terraform Drift Detection &amp; Compliance CLI</p> <p>Cloudrift compares your Terraform plan JSON against live AWS infrastructure to detect configuration drift before <code>terraform apply</code>. It evaluates 49 built-in OPA security policies and scores compliance across 5 industry frameworks \u2014 all from a single CLI command.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p> Drift Detection</p> <p>Compare live AWS resources (S3, EC2, IAM) against Terraform plan files. See attribute-level diffs with colorized console output.</p> </li> <li> <p> 49 Security Policies</p> <p>OPA-powered policy engine covering S3, EC2, RDS, IAM, Security Groups, CloudTrail, KMS, Lambda, ELB, EBS, VPC, and Secrets Manager.</p> </li> <li> <p> 5 Compliance Frameworks</p> <p>HIPAA, GDPR, ISO 27001, PCI DSS, and SOC 2 compliance scoring with per-framework breakdowns.</p> </li> <li> <p> Framework Filtering</p> <p>Focus on the frameworks that matter with <code>--frameworks=hipaa,soc2</code>. Only relevant policies are evaluated and scored.</p> </li> <li> <p> Docker &amp; CI/CD</p> <p>Run as a Docker container. Integrate into GitHub Actions or GitLab CI with <code>--fail-on-violation</code> and SARIF output.</p> </li> <li> <p> 3 Output Formats</p> <p>Console (colorized), JSON (machine-readable), and SARIF (GitHub Security tab integration).</p> </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"Go InstallDockerBuild from Source <pre><code>go install github.com/inayathulla/cloudrift@latest\n</code></pre> <pre><code>docker pull inayathulla/cloudrift:latest\ndocker run -v ~/.aws:/root/.aws:ro \\\n  -v $(pwd):/work \\\n  inayathulla/cloudrift:latest scan \\\n  --config=/work/cloudrift-s3.yml --service=s3\n</code></pre> <pre><code>git clone https://github.com/inayathulla/cloudrift.git\ncd cloudrift\ngo build -o cloudrift main.go\n</code></pre>"},{"location":"#run-your-first-scan","title":"Run Your First Scan","text":"<pre><code># 1. Generate a Terraform plan\nterraform plan -out=tfplan\nterraform show -json tfplan &gt; plan.json\n\n# 2. Create a config file\ncat &gt; cloudrift-s3.yml &lt;&lt;EOF\naws_profile: default\nregion: us-east-1\nplan_path: ./plan.json\nEOF\n\n# 3. Scan for drift and policy violations\ncloudrift scan --service=s3\n</code></pre> <p>Get Started  View on GitHub </p>"},{"location":"#sample-output","title":"Sample Output","text":"<pre><code>\ud83d\ude80 Starting Cloudrift scan...\n\ud83d\udd10 Connected as: arn:aws:iam::123456789012:root (123456789012) [us-east-1]\n\u2714\ufe0f  Evaluated 49 policies in 23ms\n\u26a0\ufe0f  Found 2 policy violations\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n              COMPLIANCE SUMMARY\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n  Overall: 95.9% (47/49 policies passing)\n\n  Categories:\n    cost         100.0% (3/3)\n    security     95.2% (40/42)\n    tagging      100.0% (4/4)\n\n  Frameworks:\n    gdpr         94.4% (17/18)\n    hipaa        96.2% (25/26)\n    iso_27001    97.4% (38/39)\n    pci_dss      97.1% (33/34)\n    soc2         97.5% (39/40)\n</code></pre>"},{"location":"#why-cloudrift","title":"Why Cloudrift?","text":"Feature Cloudrift Terraform Cloud Checkov driftctl Pre-apply drift detection Live AWS comparison OPA policy engine Sentinel Compliance scoring Framework filtering SARIF output Free &amp; open source Paid"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to Cloudrift! This guide covers the workflow for submitting changes.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li> <p>Clone your fork locally:</p> <pre><code>git clone https://github.com/YOUR_USERNAME/cloudrift.git\ncd cloudrift\n</code></pre> </li> <li> <p>Create a branch for your feature or fix:</p> <pre><code>git checkout -b feature/my-feature\n</code></pre> </li> <li> <p>Build and test to verify everything works:</p> <pre><code>go build -o cloudrift main.go\ngo test ./...\n</code></pre> </li> </ol>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#code-changes","title":"Code Changes","text":"<ol> <li>Make your changes in the appropriate package</li> <li>Format your code: <code>go fmt ./...</code></li> <li>Run tests: <code>go test ./...</code></li> <li>Build: <code>go build -o cloudrift main.go</code></li> </ol>"},{"location":"contributing/#adding-features","title":"Adding Features","text":"<ul> <li>New AWS service \u2014 See Adding Services</li> <li>New OPA policy \u2014 See Adding Policies</li> <li>New output format \u2014 Implement the <code>Formatter</code> interface in <code>internal/output/</code></li> </ul>"},{"location":"contributing/#code-standards","title":"Code Standards","text":""},{"location":"contributing/#go-conventions","title":"Go Conventions","text":"<ul> <li>Follow Effective Go guidelines</li> <li>Use <code>go fmt</code> for formatting (enforced in CI)</li> <li>Keep packages focused and small</li> <li>Use descriptive variable names</li> <li>Add comments for exported functions</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for all new functionality</li> <li>Place tests in <code>tests/internal/</code> mirroring the package structure</li> <li>Use <code>testify</code> for assertions</li> <li>Aim for table-driven tests where applicable</li> </ul>"},{"location":"contributing/#policy-conventions","title":"Policy Conventions","text":"<ul> <li>Follow the existing <code>.rego</code> file patterns</li> <li>Include all metadata fields (<code>policy_id</code>, <code>policy_name</code>, <code>msg</code>, <code>severity</code>, <code>remediation</code>, <code>category</code>, <code>frameworks</code>)</li> <li>Place in the correct category directory (<code>security/</code>, <code>tagging/</code>, <code>cost/</code>)</li> </ul>"},{"location":"contributing/#commit-messages","title":"Commit Messages","text":"<p>Use clear, concise commit messages:</p> <pre><code>Add RDS drift detection support\n\nImplements drift detection for aws_db_instance resources including\nstorage encryption, public access, and backup retention checks.\n</code></pre> <ul> <li>Use imperative mood (\"Add\" not \"Added\")</li> <li>First line: summary under 72 characters</li> <li>Optional body: explain why, not what</li> </ul>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure all tests pass: <code>go test ./...</code></li> <li>Ensure code is formatted: <code>go fmt ./...</code></li> <li>Push your branch: <code>git push origin feature/my-feature</code></li> <li>Open a pull request against <code>main</code></li> <li>Describe what changed and why</li> <li>Link any related issues</li> </ol>"},{"location":"contributing/#pr-checklist","title":"PR Checklist","text":"<ul> <li>[ ] Tests pass (<code>go test ./...</code>)</li> <li>[ ] Code is formatted (<code>go fmt ./...</code>)</li> <li>[ ] New features have tests</li> <li>[ ] New policies include all metadata fields</li> <li>[ ] Documentation updated if needed</li> </ul>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":"<p>Found a bug or have a feature request? Open an issue on GitHub.</p> <p>Include:</p> <ul> <li>Expected behavior vs actual behavior</li> <li>Steps to reproduce</li> <li>Cloudrift version (<code>cloudrift --help</code>)</li> <li>Go version (<code>go version</code>)</li> <li>OS and architecture</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the Apache License 2.0.</p>"},{"location":"architecture/data-flow/","title":"Data Flow","text":"<p>This page traces a complete scan from CLI invocation to output.</p>"},{"location":"architecture/data-flow/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>sequenceDiagram\n    participant User\n    participant CLI as cmd/scan.go\n    participant Config as Viper\n    participant AWS as AWS SDK\n    participant Parser as internal/parser\n    participant Detector as internal/detector\n    participant PolicyEngine as internal/policy\n    participant Output as internal/output\n\n    User-&gt;&gt;CLI: cloudrift scan --service=s3\n    CLI-&gt;&gt;Config: Load cloudrift-&lt;service&gt;.yml\n    Config--&gt;&gt;CLI: profile, region, plan_path\n    CLI-&gt;&gt;AWS: InitAWS(profile, region)\n    AWS--&gt;&gt;CLI: aws.Config\n    CLI-&gt;&gt;AWS: ValidateCredentials()\n    CLI-&gt;&gt;AWS: GetCallerIdentity()\n    AWS--&gt;&gt;CLI: Account ID, ARN\n\n    CLI-&gt;&gt;Parser: LoadPlan(plan_path)\n    Parser--&gt;&gt;CLI: []S3Bucket (planned)\n\n    CLI-&gt;&gt;Detector: FetchLiveState()\n    Note over Detector,AWS: Parallel API calls per bucket\n    Detector-&gt;&gt;AWS: GetBucketAcl, GetBucketTagging, ...\n    AWS--&gt;&gt;Detector: Live bucket attributes\n    Detector--&gt;&gt;CLI: []S3Bucket (live)\n\n    CLI-&gt;&gt;Detector: DetectDrift(planned, live)\n    Detector--&gt;&gt;CLI: []DriftResult\n\n    CLI-&gt;&gt;PolicyEngine: LoadBuiltinPolicies()\n    CLI-&gt;&gt;PolicyEngine: EvaluateAll(inputs)\n    PolicyEngine--&gt;&gt;CLI: EvaluationResult\n\n    CLI-&gt;&gt;Output: Format(ScanResult)\n    Output--&gt;&gt;User: Console / JSON / SARIF</code></pre>"},{"location":"architecture/data-flow/#step-by-step-flow","title":"Step-by-Step Flow","text":""},{"location":"architecture/data-flow/#1-configuration-loading","title":"1. Configuration Loading","text":"<pre><code>cloudrift-&lt;service&gt;.yml \u2192 Viper \u2192 (profile, region, plan_path)\n</code></pre> <p>Viper reads the YAML config file and extracts AWS profile, region, and Terraform plan path.</p>"},{"location":"architecture/data-flow/#2-aws-initialization","title":"2. AWS Initialization","text":"<pre><code>profile + region \u2192 AWS SDK v2 \u2192 aws.Config\naws.Config \u2192 STS GetCallerIdentity \u2192 Account ID\n</code></pre> <p>The AWS SDK is initialized with the configured profile and region. Credentials are validated via <code>GetCallerIdentity</code>.</p>"},{"location":"architecture/data-flow/#3-plan-parsing","title":"3. Plan Parsing","text":"<pre><code>plan.json \u2192 resource_changes[].change.after \u2192 []S3Bucket\n</code></pre> <p>The Terraform plan JSON is parsed to extract resources matching the selected service. Each resource's planned attributes are mapped to the service model (e.g., <code>S3Bucket</code> struct).</p>"},{"location":"architecture/data-flow/#4-live-state-fetching","title":"4. Live State Fetching","text":"<p>For S3, each bucket's attributes are fetched concurrently:</p> <pre><code>graph LR\n    subgraph \"Per Bucket (parallel)\"\n        A1[\"GetBucketAcl\"]\n        A2[\"GetBucketTagging\"]\n        A3[\"GetBucketVersioning\"]\n        A4[\"GetBucketEncryption\"]\n        A5[\"GetBucketLogging\"]\n        A6[\"GetPublicAccessBlock\"]\n        A7[\"GetLifecycleConfig\"]\n    end\n    A1 --&gt; R[\"S3Bucket struct\"]\n    A2 --&gt; R\n    A3 --&gt; R\n    A4 --&gt; R\n    A5 --&gt; R\n    A6 --&gt; R\n    A7 --&gt; R</code></pre> <p>Expected errors (e.g., <code>NoSuchTagSet</code>) are silently ignored. Unexpected errors are logged but don't stop the scan.</p>"},{"location":"architecture/data-flow/#5-drift-detection","title":"5. Drift Detection","text":"<pre><code>planned[]  \u2500\u2500\u2510\n              \u251c\u2500\u2500 Compare attributes \u2192 DriftResult[]\nlive[]     \u2500\u2500\u2518\n</code></pre> <p>For each resource, the detector compares planned vs live attributes:</p> <ul> <li>Missing: Resource in plan but not in AWS</li> <li>Diffs: Attribute values differ (e.g., versioning enabled vs disabled)</li> <li>Extra tags: Tags in AWS not in the plan</li> </ul>"},{"location":"architecture/data-flow/#6-policy-evaluation","title":"6. Policy Evaluation","text":"<pre><code>Resources \u2192 PolicyInput[] \u2192 OPA Compiler \u2192 deny[] + warn[]\n</code></pre> <p>Resources are converted to OPA inputs. The compiler evaluates all <code>.rego</code> modules:</p> <ul> <li><code>deny</code> rules produce violations (blocking)</li> <li><code>warn</code> rules produce warnings (advisory)</li> </ul> <p>Each violation includes policy ID, name, severity, remediation, category, and framework mappings.</p>"},{"location":"architecture/data-flow/#7-output-formatting","title":"7. Output Formatting","text":"<p>The formatter converts results to the requested format:</p> Format Handler Output <code>console</code> Legacy printer + <code>printPolicyResults()</code> Colorized terminal output <code>json</code> <code>JSONFormatter</code> Structured JSON <code>sarif</code> <code>SARIFFormatter</code> SARIF 2.1.0 JSON"},{"location":"architecture/data-flow/#error-handling-strategy","title":"Error Handling Strategy","text":"Error Type Behavior Config file not found Exit with error Invalid AWS credentials Exit with error Plan file parse error Exit with error AWS API rate limit Retry (SDK built-in) Missing optional attribute (NoSuchTagSet) Silently ignore Individual resource fetch error Log warning, continue scan Policy compilation error Log warning, skip policies Policy evaluation error Log warning, skip policies"},{"location":"architecture/deployment/","title":"Deployment","text":""},{"location":"architecture/deployment/#docker","title":"Docker","text":"<p>The Docker image uses a 2-stage multi-stage build to produce a minimal Alpine runtime container.</p>"},{"location":"architecture/deployment/#build-stages","title":"Build Stages","text":"<pre><code>graph LR\n    S1[\"Stage 1&lt;br/&gt;golang:1.24&lt;br/&gt;Build\"] --&gt; S2[\"Stage 2&lt;br/&gt;alpine:latest&lt;br/&gt;Runtime\"]</code></pre>"},{"location":"architecture/deployment/#stage-1-build","title":"Stage 1: Build","text":"<pre><code>FROM golang:1.24 AS build\nENV CGO_ENABLED=0 GOOS=linux GOARCH=amd64\n# Downloads dependencies, compiles static binary\n# Output: /app/cloudrift\n</code></pre> <p>Produces a statically-linked Linux binary with <code>CGO_ENABLED=0</code> for maximum portability.</p>"},{"location":"architecture/deployment/#stage-2-runtime","title":"Stage 2: Runtime","text":"<pre><code>FROM alpine:latest\n# Creates non-root 'cloudrift' user\n# Copies binary from build stage\n# ENTRYPOINT [\"/cloudrift\"]\n</code></pre> <p>The final image is minimal \u2014 only Alpine + the compiled binary, running as a non-root user.</p>"},{"location":"architecture/deployment/#building-the-image","title":"Building the Image","text":"<pre><code>docker build -t cloudrift .\n</code></pre>"},{"location":"architecture/deployment/#running","title":"Running","text":"<pre><code>docker run --rm \\\n  -v ~/.aws:/root/.aws:ro \\\n  -v $(pwd):/work \\\n  cloudrift scan \\\n  --config=/work/cloudrift-s3.yml \\\n  --service=s3\n</code></pre> <p>AWS credentials</p> <p>Mount <code>~/.aws</code> as read-only (<code>:ro</code>) to provide credentials. Never bake credentials into the image.</p>"},{"location":"architecture/deployment/#docker-hub","title":"Docker Hub","text":"<pre><code>docker pull inayathulla/cloudrift:latest\n</code></pre> Tag Description <code>latest</code> Latest stable release <code>v1.0.0</code> Specific version"},{"location":"architecture/deployment/#go-binary","title":"Go Binary","text":""},{"location":"architecture/deployment/#go-install","title":"go install","text":"<pre><code>go install github.com/inayathulla/cloudrift@latest\n</code></pre> <p>The binary is installed to <code>$GOPATH/bin/cloudrift</code> (typically <code>~/go/bin/cloudrift</code>).</p>"},{"location":"architecture/deployment/#build-from-source","title":"Build from Source","text":"<pre><code>git clone https://github.com/inayathulla/cloudrift.git\ncd cloudrift\ngo build -o cloudrift main.go\n</code></pre>"},{"location":"architecture/deployment/#cross-compilation","title":"Cross-Compilation","text":"<p>Build for different platforms:</p> <pre><code># Linux (amd64)\nGOOS=linux GOARCH=amd64 go build -o cloudrift-linux main.go\n\n# macOS (Apple Silicon)\nGOOS=darwin GOARCH=arm64 go build -o cloudrift-darwin main.go\n\n# Windows\nGOOS=windows GOARCH=amd64 go build -o cloudrift.exe main.go\n</code></pre>"},{"location":"architecture/deployment/#cicd-pipeline-deployment","title":"CI/CD Pipeline Deployment","text":"<p>Cloudrift is designed for CI/CD integration. See CI/CD Integration for complete GitHub Actions and GitLab CI workflows.</p>"},{"location":"architecture/deployment/#quick-reference","title":"Quick Reference","text":"<pre><code># Install in CI\ngo install github.com/inayathulla/cloudrift@latest\n\n# Or use Docker\ndocker run --rm \\\n  -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\\n  -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\\n  -v $(pwd):/work \\\n  inayathulla/cloudrift:latest scan \\\n  --config=/work/cloudrift-s3.yml \\\n  --service=s3 \\\n  --fail-on-violation \\\n  --no-emoji\n</code></pre>"},{"location":"architecture/deployment/#github-pages-documentation","title":"GitHub Pages (Documentation)","text":"<p>The documentation site is built with MkDocs Material and can be deployed via GitHub Actions:</p> <pre><code>name: Deploy Docs\non:\n  push:\n    branches: [main]\n    paths: ['docs/**', 'mkdocs.yml']\n\npermissions:\n  contents: write\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.x'\n      - run: pip install mkdocs-material mkdocs-minify-plugin\n      - run: mkdocs gh-deploy --force\n</code></pre> <p>Build locally with:</p> <pre><code>mkdocs build --strict\nmkdocs serve  # Preview at http://localhost:8000\n</code></pre>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>Cloudrift follows a pipeline architecture where each scan step feeds into the next. The codebase uses Go's modular package system with clear separation between CLI, parsing, AWS integration, detection, policy evaluation, and output formatting.</p>"},{"location":"architecture/overview/#pipeline-diagram","title":"Pipeline Diagram","text":"<pre><code>graph LR\n    A[\"CLI Entry&lt;br/&gt;cmd/scan.go\"] --&gt; B[\"Config&lt;br/&gt;Viper + AWS SDK\"]\n    B --&gt; C[\"Parse Plan&lt;br/&gt;internal/parser/\"]\n    B --&gt; D[\"Fetch Live State&lt;br/&gt;internal/aws/\"]\n    C --&gt; E[\"Detect Drift&lt;br/&gt;internal/detector/\"]\n    D --&gt; E\n    E --&gt; F[\"Evaluate Policies&lt;br/&gt;internal/policy/\"]\n    F --&gt; G[\"Format Output&lt;br/&gt;internal/output/\"]</code></pre>"},{"location":"architecture/overview/#tech-stack","title":"Tech Stack","text":"Component Technology Version Purpose Language Go 1.24 Core language CLI Framework Cobra 1.10 Command-line interface Config Viper 1.21 YAML configuration loading AWS SDK AWS SDK for Go v2 1.41 S3, EC2, IAM, STS API calls Policy Engine Open Policy Agent 1.13 OPA/Rego policy evaluation Testing Testify 1.11 Assertion library Console fatih/color 1.16 Colorized terminal output Progress briandowns/spinner 1.23 CLI progress indicators Concurrency golang.org/x/sync 0.19 errgroup for parallel API calls"},{"location":"architecture/overview/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"architecture/overview/#pre-apply-detection","title":"Pre-Apply Detection","text":"<p>Unlike post-apply tools (driftctl, Terraform Cloud drift detection), Cloudrift operates on the Terraform plan file \u2014 before any changes are applied. This enables:</p> <ul> <li>CI/CD gating before deployment</li> <li>Code review with drift context</li> <li>Zero risk of modifying infrastructure</li> </ul>"},{"location":"architecture/overview/#parallel-aws-api-calls","title":"Parallel AWS API Calls","text":"<p>S3 bucket attributes are fetched concurrently using <code>errgroup.WithContext</code>. Each bucket triggers 7 parallel API calls (ACL, tags, versioning, encryption, logging, public access block, lifecycle), reducing latency.</p> <pre><code>graph TD\n    B[\"Bucket: my-bucket\"] --&gt; A1[\"GetBucketAcl\"]\n    B --&gt; A2[\"GetBucketTagging\"]\n    B --&gt; A3[\"GetBucketVersioning\"]\n    B --&gt; A4[\"GetBucketEncryption\"]\n    B --&gt; A5[\"GetBucketLogging\"]\n    B --&gt; A6[\"GetPublicAccessBlock\"]\n    B --&gt; A7[\"GetLifecycleConfig\"]</code></pre>"},{"location":"architecture/overview/#embedded-policies","title":"Embedded Policies","text":"<p>All 49 <code>.rego</code> policy files are embedded in the binary via Go's <code>//go:embed</code> directive. This means:</p> <ul> <li>No external files required at runtime</li> <li>Policies versioned with the binary</li> <li>Custom policies can be loaded alongside built-ins via <code>--policy-dir</code></li> </ul>"},{"location":"architecture/overview/#dynamic-policy-registry","title":"Dynamic Policy Registry","text":"<p>Policy metadata (IDs, categories, frameworks) is extracted from <code>.rego</code> files at runtime using regex \u2014 never hardcoded. This ensures counts stay accurate as policies are added.</p>"},{"location":"architecture/overview/#service-based-modularity","title":"Service-Based Modularity","text":"<p>Each AWS service follows the same pattern with dedicated files:</p> Component S3 File EC2 File IAM File Parser <code>internal/parser/s3.go</code> <code>internal/parser/ec2.go</code> <code>internal/parser/iam.go</code> AWS Client <code>internal/aws/s3.go</code> <code>internal/aws/ec2.go</code> <code>internal/aws/iam.go</code> Detector <code>internal/detector/s3.go</code> <code>internal/detector/ec2.go</code> <code>internal/detector/iam.go</code> Printer <code>internal/detector/s3_printer.go</code> <code>internal/detector/ec2_printer.go</code> <code>internal/detector/iam_printer.go</code> Model <code>internal/models/s3.go</code> <code>internal/models/ec2.go</code> <code>internal/models/iam.go</code> <p>Adding a new service means creating one file per component and registering it in <code>cmd/scan.go</code>.</p>"},{"location":"architecture/overview/#graceful-error-handling","title":"Graceful Error Handling","text":"<p>AWS API calls for optional attributes (tags, lifecycle, etc.) gracefully handle \"not found\" errors. For example, <code>NoSuchTagSet</code> or <code>NoSuchLifecycleConfiguration</code> are expected for buckets without those features configured \u2014 these are silently ignored rather than causing scan failures.</p>"},{"location":"architecture/project-structure/","title":"Project Structure","text":"<pre><code>cloudrift/\n\u251c\u2500\u2500 main.go                         # Entry point\n\u251c\u2500\u2500 cmd/\n\u2502   \u251c\u2500\u2500 root.go                     # Base Cobra command\n\u2502   \u2514\u2500\u2500 scan.go                     # Scan command with all flags and pipeline logic\n\u251c\u2500\u2500 internal/\n\u2502   \u251c\u2500\u2500 aws/                        # AWS API integrations\n\u2502   \u2502   \u251c\u2500\u2500 config.go               # AWS SDK v2 configuration\n\u2502   \u2502   \u251c\u2500\u2500 s3.go                   # S3 API client (parallel attribute fetching)\n\u2502   \u2502   \u251c\u2500\u2500 ec2.go                  # EC2 API client (pagination support)\n\u2502   \u2502   \u251c\u2500\u2500 iam.go                  # IAM API client (roles, users, policies, groups)\n\u2502   \u2502   \u2514\u2500\u2500 identity.go            # STS identity operations\n\u2502   \u251c\u2500\u2500 common/                     # Shared utilities\n\u2502   \u2502   \u2514\u2500\u2500 bootstrap.go           # Config loading, AWS init, credential validation\n\u2502   \u251c\u2500\u2500 detector/                   # Drift detection logic\n\u2502   \u2502   \u251c\u2500\u2500 interface.go           # DriftResultPrinter interface\n\u2502   \u2502   \u251c\u2500\u2500 registry.go            # Service detector registry\n\u2502   \u2502   \u251c\u2500\u2500 s3.go                  # S3 drift detector\n\u2502   \u2502   \u251c\u2500\u2500 ec2.go                 # EC2 drift detector\n\u2502   \u2502   \u251c\u2500\u2500 iam.go                 # IAM drift detector\n\u2502   \u2502   \u251c\u2500\u2500 s3_printer.go          # S3 console output\n\u2502   \u2502   \u251c\u2500\u2500 ec2_printer.go         # EC2 console output\n\u2502   \u2502   \u251c\u2500\u2500 iam_printer.go         # IAM console output\n\u2502   \u2502   \u2514\u2500\u2500 printer.go            # Common printer utilities\n\u2502   \u251c\u2500\u2500 models/                     # Data structures\n\u2502   \u2502   \u251c\u2500\u2500 s3.go                  # S3Bucket, PublicAccessBlockConfig, LifecycleRuleSummary\n\u2502   \u2502   \u251c\u2500\u2500 ec2.go                 # EC2Instance, BlockDevice\n\u2502   \u2502   \u251c\u2500\u2500 iam.go                 # IAMRole, IAMUser, IAMPolicy, IAMGroup\n\u2502   \u2502   \u2514\u2500\u2500 analytics.go          # Analytics models\n\u2502   \u251c\u2500\u2500 output/                     # Output formatters\n\u2502   \u2502   \u251c\u2500\u2500 formatter.go          # Format registry, interfaces, data types\n\u2502   \u2502   \u251c\u2500\u2500 console.go            # Colorized CLI formatter\n\u2502   \u2502   \u251c\u2500\u2500 json.go               # JSON formatter\n\u2502   \u2502   \u2514\u2500\u2500 sarif.go              # SARIF 2.1.0 formatter\n\u2502   \u251c\u2500\u2500 parser/                     # Terraform plan JSON parsers\n\u2502   \u2502   \u251c\u2500\u2500 plan.go               # Core parsing logic\n\u2502   \u2502   \u251c\u2500\u2500 s3.go                 # S3 resource parser\n\u2502   \u2502   \u251c\u2500\u2500 ec2.go                # EC2 resource parser\n\u2502   \u2502   \u2514\u2500\u2500 iam.go                # IAM resource parser\n\u2502   \u2514\u2500\u2500 policy/                     # OPA policy engine\n\u2502       \u251c\u2500\u2500 engine.go             # Policy evaluation (compile, query, parse)\n\u2502       \u251c\u2500\u2500 loader.go             # Embedded policy loading (//go:embed)\n\u2502       \u251c\u2500\u2500 registry.go           # Dynamic policy metadata extraction\n\u2502       \u251c\u2500\u2500 result.go             # Violation, EvaluationResult structs\n\u2502       \u251c\u2500\u2500 input.go              # PolicyInput structs\n\u2502       \u2514\u2500\u2500 policies/             # 49 built-in OPA policies\n\u2502           \u251c\u2500\u2500 security/         # 42 security policies (16 .rego files)\n\u2502           \u251c\u2500\u2500 tagging/          # 4 tagging policies (1 .rego file)\n\u2502           \u2514\u2500\u2500 cost/             # 3 cost policies (1 .rego file)\n\u251c\u2500\u2500 tests/                          # Unit test suite\n\u2502   \u2514\u2500\u2500 internal/\n\u2502       \u251c\u2500\u2500 detector/             # Drift detection tests\n\u2502       \u251c\u2500\u2500 models/               # Model tests\n\u2502       \u251c\u2500\u2500 output/               # Formatter tests\n\u2502       \u251c\u2500\u2500 parser/               # Plan parser tests\n\u2502       \u2514\u2500\u2500 policy/               # Policy engine + registry tests\n\u251c\u2500\u2500 config/                         # Example configurations\n\u2502   \u251c\u2500\u2500 cloudrift-s3.yml            # S3 scanning config\n\u2502   \u251c\u2500\u2500 cloudrift-ec2.yml          # EC2 scanning config\n\u2502   \u2514\u2500\u2500 cloudrift-iam.yml          # IAM scanning config\n\u251c\u2500\u2500 docs/                           # MkDocs documentation\n\u251c\u2500\u2500 .github/workflows/             # CI/CD workflows\n\u251c\u2500\u2500 Dockerfile                      # Multi-stage Docker build\n\u251c\u2500\u2500 go.mod                          # Go module definition\n\u2514\u2500\u2500 go.sum                          # Dependency lock file\n</code></pre>"},{"location":"architecture/project-structure/#package-responsibilities","title":"Package Responsibilities","text":"Package Responsibility <code>cmd</code> CLI commands, flag parsing, scan pipeline orchestration, compliance scoring <code>internal/aws</code> AWS SDK v2 API calls for each service <code>internal/common</code> Shared utilities: config loading, AWS initialization, credential validation <code>internal/detector</code> Drift detection logic: compare planned vs live, build DriftResult structs <code>internal/models</code> Data structures for AWS resources (S3Bucket, EC2Instance, IAMRole, etc.) <code>internal/output</code> Output formatters (Console, JSON, SARIF) and format registry <code>internal/parser</code> Terraform plan JSON parsing, resource extraction <code>internal/policy</code> OPA policy engine: loading, compilation, evaluation, result types <code>tests</code> Unit tests mirroring the <code>internal/</code> package structure"},{"location":"architecture/project-structure/#key-interfaces","title":"Key Interfaces","text":""},{"location":"architecture/project-structure/#driftdetector","title":"DriftDetector","text":"<pre><code>type DriftDetector interface {\n    FetchLiveState() (interface{}, error)\n    DetectDrift(plan interface{}, live interface{}) ([]DriftResult, error)\n}\n</code></pre> <p>Implemented by <code>S3DriftDetector</code>, <code>EC2DriftDetector</code>, and <code>IAMDriftDetector</code>.</p>"},{"location":"architecture/project-structure/#driftresultprinter","title":"DriftResultPrinter","text":"<pre><code>type DriftResultPrinter interface {\n    PrintDrift(results []DriftResult, plan interface{}, live interface{})\n}\n</code></pre> <p>Implemented by <code>S3DriftResultPrinter</code>, <code>EC2DriftResultPrinter</code>, and <code>IAMDriftResultPrinter</code>.</p>"},{"location":"architecture/project-structure/#formatter","title":"Formatter","text":"<pre><code>type Formatter interface {\n    Format(w io.Writer, result ScanResult) error\n    Name() string\n    FileExtension() string\n}\n</code></pre> <p>Implemented by <code>JSONFormatter</code>, <code>SARIFFormatter</code>, and <code>ConsoleFormatter</code>.</p>"},{"location":"architecture/project-structure/#key-data-models","title":"Key Data Models","text":""},{"location":"architecture/project-structure/#s3bucket","title":"S3Bucket","text":"<pre><code>type S3Bucket struct {\n    Id                  string\n    Name                string\n    Acl                 string\n    Tags                map[string]string\n    VersioningEnabled   bool\n    EncryptionAlgorithm string\n    LoggingEnabled      bool\n    PublicAccessBlock   PublicAccessBlockConfig\n    LifecycleRules      []LifecycleRuleSummary\n}\n</code></pre>"},{"location":"architecture/project-structure/#ec2instance","title":"EC2Instance","text":"<pre><code>type EC2Instance struct {\n    InstanceID         string\n    InstanceType       string\n    AMI                string\n    SubnetID           string\n    Tags               map[string]string\n    EBSOptimized       bool\n    Monitoring         bool\n    RootBlockDevice    BlockDevice\n    TerraformAddress   string\n}\n</code></pre>"},{"location":"architecture/project-structure/#violation","title":"Violation","text":"<pre><code>type Violation struct {\n    PolicyID        string\n    PolicyName      string\n    Message         string\n    Severity        Severity    // critical, high, medium, low, info\n    ResourceType    string\n    ResourceAddress string\n    Remediation     string\n    Category        string      // security, tagging, cost\n    Frameworks      []string    // hipaa, pci_dss, iso_27001, gdpr, soc2\n}\n</code></pre>"},{"location":"cli/output-formats/","title":"Output Formats","text":"<p>Cloudrift supports three output formats: Console, JSON, and SARIF.</p>"},{"location":"cli/output-formats/#console-default","title":"Console (Default)","text":"<p>The default format produces colorized terminal output with emojis, drift details, policy violations, and compliance scoring.</p> <pre><code>cloudrift scan --service=s3\n</code></pre>"},{"location":"cli/output-formats/#sample-output","title":"Sample Output","text":"<pre><code>\ud83d\ude80 Starting Cloudrift scan...\n\u2714\ufe0f  AWS config loaded in 45ms\n\u2714\ufe0f  Credentials valided in 120ms\n\ud83d\udd10 Connected as: arn:aws:iam::123456789012:root (123456789012) [us-east-1] in 89ms\n\ud83d\udcc4 Plan loaded from json in 5ms\n\u2714\ufe0f  Live S3 state fetched in 234ms\n\u2714\ufe0f  Drift detection completed\n\u2714\ufe0f  Evaluated 49 policies in 23ms\n\u26a0\ufe0f  Found 2 policy violations\n\u2714\ufe0f  Scan completed in 516ms!\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n              POLICY EVALUATION\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n\u274c VIOLATIONS (2)\n\n  [high] S3-001\n  \ud83d\udccd Resource: aws_s3_bucket.data\n  \ud83d\udcac S3 bucket must have server-side encryption enabled\n  \ud83d\udd27 Add server_side_encryption_configuration block\n\n  [medium] S3-009\n  \ud83d\udccd Resource: aws_s3_bucket.data\n  \ud83d\udcac S3 bucket does not have versioning enabled\n  \ud83d\udd27 Enable versioning in aws_s3_bucket_versioning resource\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n            COMPLIANCE SUMMARY\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n  Overall: 95.9% (47/49 policies passing)\n\n  Categories:\n    cost         100.0% (3/3)\n    security     95.2% (40/42)\n    tagging      100.0% (4/4)\n\n  Frameworks:\n    gdpr         94.4% (17/18)\n    hipaa        96.2% (25/26)\n    iso_27001    97.4% (38/39)\n    pci_dss      97.1% (33/34)\n    soc2         97.5% (39/40)\n</code></pre> <p>ASCII mode</p> <p>Use <code>--no-emoji</code> for CI/CD environments that don't render Unicode emojis.</p>"},{"location":"cli/output-formats/#json","title":"JSON","text":"<p>Machine-readable JSON output with full drift details, policy results, and compliance scoring.</p> <pre><code>cloudrift scan --service=s3 --format=json\n</code></pre>"},{"location":"cli/output-formats/#sample-output_1","title":"Sample Output","text":"<pre><code>{\n  \"service\": \"s3\",\n  \"account_id\": \"123456789012\",\n  \"region\": \"us-east-1\",\n  \"total_resources\": 3,\n  \"drift_count\": 1,\n  \"drifts\": [\n    {\n      \"resource_id\": \"my-bucket\",\n      \"resource_type\": \"aws_s3_bucket\",\n      \"resource_name\": \"my-bucket\",\n      \"missing\": false,\n      \"diffs\": {\n        \"versioning_enabled\": [\"true\", \"false\"]\n      },\n      \"severity\": \"warning\"\n    }\n  ],\n  \"policy_result\": {\n    \"violations\": [\n      {\n        \"policy_id\": \"S3-001\",\n        \"policy_name\": \"S3 Encryption Required\",\n        \"message\": \"S3 bucket must have server-side encryption enabled\",\n        \"severity\": \"high\",\n        \"resource_type\": \"aws_s3_bucket\",\n        \"resource_address\": \"aws_s3_bucket.data\",\n        \"remediation\": \"Add server_side_encryption_configuration block\",\n        \"category\": \"security\",\n        \"frameworks\": [\"hipaa\", \"pci_dss\", \"iso_27001\", \"gdpr\", \"soc2\"]\n      }\n    ],\n    \"warnings\": [],\n    \"passed\": 48,\n    \"failed\": 1,\n    \"compliance\": {\n      \"overall_percentage\": 97.96,\n      \"total_policies\": 49,\n      \"passing_policies\": 48,\n      \"failing_policies\": 1,\n      \"categories\": {\n        \"security\": { \"percentage\": 97.62, \"passed\": 41, \"failed\": 1, \"total\": 42 },\n        \"tagging\": { \"percentage\": 100, \"passed\": 4, \"failed\": 0, \"total\": 4 },\n        \"cost\": { \"percentage\": 100, \"passed\": 3, \"failed\": 0, \"total\": 3 }\n      },\n      \"frameworks\": {\n        \"hipaa\": { \"percentage\": 96.15, \"passed\": 25, \"failed\": 1, \"total\": 26 },\n        \"pci_dss\": { \"percentage\": 97.06, \"passed\": 33, \"failed\": 1, \"total\": 34 },\n        \"iso_27001\": { \"percentage\": 97.44, \"passed\": 38, \"failed\": 1, \"total\": 39 },\n        \"gdpr\": { \"percentage\": 94.44, \"passed\": 17, \"failed\": 1, \"total\": 18 },\n        \"soc2\": { \"percentage\": 97.5, \"passed\": 39, \"failed\": 1, \"total\": 40 }\n      },\n      \"active_frameworks\": [\"hipaa\", \"soc2\"]\n    }\n  },\n  \"scan_duration_ms\": 516,\n  \"timestamp\": \"2024-02-14T10:30:00Z\"\n}\n</code></pre> <p><code>active_frameworks</code></p> <p>The <code>active_frameworks</code> field only appears when <code>--frameworks</code> is set. It tells downstream tools which frameworks were selected.</p>"},{"location":"cli/output-formats/#sarif","title":"SARIF","text":"<p>Static Analysis Results Interchange Format for integration with GitHub Code Scanning, GitLab SAST, and other tools.</p> <pre><code>cloudrift scan --service=s3 --format=sarif --output=results.sarif\n</code></pre> <p>SARIF output follows the SARIF 2.1.0 specification and includes:</p> <ul> <li>Rules \u2014 Drift detection rules (DRIFT001, DRIFT002, DRIFT003)</li> <li>Results \u2014 Individual drift findings with severity mapping</li> <li>Tool information \u2014 Cloudrift version and description</li> </ul>"},{"location":"cli/output-formats/#github-integration","title":"GitHub Integration","text":"<p>Upload SARIF results to GitHub's Security tab:</p> <pre><code>- uses: github/codeql-action/upload-sarif@v3\n  with:\n    sarif_file: results.sarif\n</code></pre>"},{"location":"cli/output-formats/#writing-to-files","title":"Writing to Files","text":"<p>Use <code>--output</code> to write to a file instead of stdout:</p> <pre><code># JSON report\ncloudrift scan --service=s3 --format=json --output=report.json\n\n# SARIF for CI\ncloudrift scan --service=s3 --format=sarif --output=drift.sarif\n</code></pre> <p>When <code>--output</code> is used with console format, the colorized output is still written to the terminal while the structured output goes to the file.</p>"},{"location":"cli/scan-command/","title":"Scan Command","text":"<p>The <code>scan</code> command is Cloudrift's primary command. It compares Terraform plan JSON against live AWS infrastructure and evaluates security policies.</p>"},{"location":"cli/scan-command/#usage","title":"Usage","text":"<pre><code>cloudrift scan [flags]\n</code></pre>"},{"location":"cli/scan-command/#flags","title":"Flags","text":"Flag Short Type Default Description <code>--config</code> <code>-c</code> string <code>cloudrift-s3.yml</code> Path to configuration file <code>--service</code> <code>-s</code> string <code>s3</code> AWS service to scan (<code>s3</code>, <code>ec2</code>, <code>iam</code>) <code>--format</code> <code>-f</code> string <code>console</code> Output format (<code>console</code>, <code>json</code>, <code>sarif</code>) <code>--output</code> <code>-o</code> string stdout Write output to file instead of stdout <code>--policy-dir</code> <code>-p</code> string \u2014 Directory containing custom OPA policies <code>--frameworks</code> \u2014 string all Comma-separated compliance frameworks (<code>hipaa,soc2,gdpr,pci_dss,iso_27001</code>) <code>--fail-on-violation</code> \u2014 bool <code>false</code> Exit with non-zero code if policy violations found <code>--skip-policies</code> \u2014 bool <code>false</code> Skip policy evaluation (drift detection only) <code>--no-emoji</code> \u2014 bool <code>false</code> Use ASCII characters instead of emojis"},{"location":"cli/scan-command/#examples","title":"Examples","text":""},{"location":"cli/scan-command/#basic-scans","title":"Basic Scans","text":"<pre><code># Scan S3 buckets (default)\ncloudrift scan --service=s3\n\n# Scan EC2 instances\ncloudrift scan --service=ec2\n\n# Scan IAM resources (roles, users, policies, groups)\ncloudrift scan --service=iam\n\n# Use a custom config\ncloudrift scan --config=/path/to/cloudrift-s3.yml --service=s3\n</code></pre>"},{"location":"cli/scan-command/#output-formats","title":"Output Formats","text":"<pre><code># JSON output to stdout\ncloudrift scan --service=s3 --format=json\n\n# SARIF output to file\ncloudrift scan --service=s3 --format=sarif --output=results.sarif\n\n# JSON output to file\ncloudrift scan --service=s3 --format=json --output=report.json\n</code></pre>"},{"location":"cli/scan-command/#framework-filtering","title":"Framework Filtering","text":"<pre><code># HIPAA-only compliance\ncloudrift scan --service=s3 --frameworks=hipaa\n\n# Multiple frameworks\ncloudrift scan --service=s3 --frameworks=hipaa,gdpr\n\n# SOC 2 + PCI DSS with JSON output\ncloudrift scan --service=s3 --format=json --frameworks=soc2,pci_dss\n</code></pre>"},{"location":"cli/scan-command/#cicd-usage","title":"CI/CD Usage","text":"<pre><code># Fail pipeline on violations\ncloudrift scan --service=s3 --fail-on-violation\n\n# SARIF for GitHub Security tab\ncloudrift scan --service=s3 --format=sarif --output=results.sarif --fail-on-violation\n\n# ASCII output for CI logs (no emojis)\ncloudrift scan --service=s3 --no-emoji --fail-on-violation\n</code></pre>"},{"location":"cli/scan-command/#custom-policies","title":"Custom Policies","text":"<pre><code># Use custom policies alongside built-ins\ncloudrift scan --service=s3 --policy-dir=./my-policies\n\n# Skip all policies (drift detection only)\ncloudrift scan --service=s3 --skip-policies\n</code></pre>"},{"location":"cli/scan-command/#exit-codes","title":"Exit Codes","text":"Code Meaning <code>0</code> Scan completed successfully, no violations (or <code>--fail-on-violation</code> not set) <code>1</code> Error (invalid config, AWS credentials, plan file, etc.) <code>2</code> Policy violations found (requires <code>--fail-on-violation</code>)"},{"location":"cli/scan-command/#scan-pipeline","title":"Scan Pipeline","text":"<p>The scan command executes in 8 sequential steps:</p> <ol> <li>Load config \u2014 Read <code>cloudrift-&lt;service&gt;.yml</code> via Viper</li> <li>Initialize AWS \u2014 Load AWS SDK v2 config with profile and region</li> <li>Validate credentials \u2014 Verify AWS credentials are valid</li> <li>Fetch identity \u2014 Call STS <code>GetCallerIdentity</code> to display account info</li> <li>Load plan \u2014 Parse Terraform plan JSON for the selected service</li> <li>Fetch live state \u2014 Query AWS APIs for current resource state</li> <li>Detect drift \u2014 Compare planned vs live attributes</li> <li>Evaluate policies \u2014 Run OPA policies against resources and output results</li> </ol> <p>Each step displays a progress spinner and elapsed time.</p>"},{"location":"cli/scan-command/#framework-validation","title":"Framework Validation","text":"<p>When <code>--frameworks</code> is set, Cloudrift validates the specified framework names against its known list. Unknown names cause an error with the list of valid options:</p> <pre><code>\u274c Unknown framework: hipa\n  Available frameworks: gdpr, hipaa, iso_27001, pci_dss, soc2\n</code></pre>"},{"location":"development/adding-policies/","title":"Adding a New Policy","text":"<p>This guide explains how to write and register a new OPA policy in Cloudrift.</p>"},{"location":"development/adding-policies/#policy-structure","title":"Policy Structure","text":"<p>Each <code>.rego</code> file contains one or more policy rules. Every rule returns a result object with required metadata fields.</p>"},{"location":"development/adding-policies/#template","title":"Template","text":"<p>Create a new <code>.rego</code> file in the appropriate category directory:</p> <pre><code>internal/policy/policies/\n\u251c\u2500\u2500 security/     \u2190 Encryption, access control, network, IAM, audit\n\u251c\u2500\u2500 tagging/      \u2190 Resource tagging governance\n\u2514\u2500\u2500 cost/         \u2190 Instance sizing, generation optimization\n</code></pre>"},{"location":"development/adding-policies/#example-new-s3-policy","title":"Example: New S3 Policy","text":"<p>Create <code>internal/policy/policies/security/s3_lifecycle.rego</code>:</p> <pre><code>package cloudrift.security.s3_lifecycle\n\n# S3 Lifecycle Policy Required\ndeny[result] {\n    input.resource.type == \"aws_s3_bucket\"\n    planned := input.resource.planned\n\n    # Check if lifecycle rules are missing or empty\n    not planned.lifecycle_rules\n\n    result := {\n        \"policy_id\": \"S3-010\",\n        \"policy_name\": \"S3 Lifecycle Policy Required\",\n        \"msg\": sprintf(\"S3 bucket '%s' does not have lifecycle rules configured\", [input.resource.address]),\n        \"severity\": \"medium\",\n        \"remediation\": \"Add aws_s3_bucket_lifecycle_configuration with appropriate transition and expiration rules\",\n        \"category\": \"security\",\n        \"frameworks\": [\"iso_27001\", \"soc2\"],\n    }\n}\n</code></pre>"},{"location":"development/adding-policies/#required-fields","title":"Required Fields","text":"<p>Every policy result must include:</p> Field Type Required Example <code>policy_id</code> string yes <code>\"S3-010\"</code> <code>policy_name</code> string yes <code>\"S3 Lifecycle Policy Required\"</code> <code>msg</code> string yes <code>\"S3 bucket 'my-bucket' does not have...\"</code> <code>severity</code> string yes <code>\"critical\"</code>, <code>\"high\"</code>, <code>\"medium\"</code>, <code>\"low\"</code> <code>remediation</code> string recommended Fix guidance <code>category</code> string recommended <code>\"security\"</code>, <code>\"tagging\"</code>, <code>\"cost\"</code> <code>frameworks</code> array recommended <code>[\"hipaa\", \"pci_dss\", \"iso_27001\", \"gdpr\", \"soc2\"]</code>"},{"location":"development/adding-policies/#rule-types","title":"Rule Types","text":""},{"location":"development/adding-policies/#deny-rules-violations","title":"<code>deny</code> Rules (Violations)","text":"<p>Blocking findings that count as policy failures:</p> <pre><code>deny[result] {\n    # condition\n    result := { ... }\n}\n</code></pre>"},{"location":"development/adding-policies/#warn-rules-warnings","title":"<code>warn</code> Rules (Warnings)","text":"<p>Advisory findings that don't affect compliance scoring:</p> <pre><code>warn[result] {\n    # condition\n    result := { ... }\n}\n</code></pre>"},{"location":"development/adding-policies/#available-frameworks","title":"Available Frameworks","text":"Key Framework <code>hipaa</code> HIPAA <code>gdpr</code> GDPR <code>iso_27001</code> ISO 27001 <code>pci_dss</code> PCI DSS <code>soc2</code> SOC 2 Type II"},{"location":"development/adding-policies/#policy-id-conventions","title":"Policy ID Conventions","text":"Prefix Service <code>S3-</code> S3 buckets <code>EC2-</code> EC2 instances <code>SG-</code> Security groups <code>RDS-</code> RDS databases <code>IAM-</code> IAM policies and roles <code>CT-</code> CloudTrail <code>KMS-</code> KMS keys <code>ELB-</code> Load balancers <code>EBS-</code> EBS volumes <code>LAMBDA-</code> Lambda functions <code>LOG-</code> CloudWatch logs <code>VPC-</code> VPC resources <code>SECRET-</code> Secrets Manager <code>TAG-</code> Tagging rules <code>COST-</code> Cost optimization"},{"location":"development/adding-policies/#input-structure","title":"Input Structure","text":"<p>Policies receive this input structure:</p> <pre><code>{\n  \"resource\": {\n    \"type\": \"aws_s3_bucket\",\n    \"address\": \"aws_s3_bucket.my_bucket\",\n    \"planned\": {\n      \"bucket\": \"my-bucket\",\n      \"acl\": \"private\",\n      \"tags\": { \"Environment\": \"prod\" },\n      \"versioning_enabled\": true,\n      \"encryption_algorithm\": \"AES256\"\n    },\n    \"drift\": {\n      \"has_drift\": false,\n      \"missing\": false\n    }\n  }\n}\n</code></pre>"},{"location":"development/adding-policies/#testing-your-policy","title":"Testing Your Policy","text":""},{"location":"development/adding-policies/#with-custom-policy-directory","title":"With Custom Policy Directory","text":"<pre><code>cloudrift scan --service=s3 --policy-dir=./my-policies\n</code></pre>"},{"location":"development/adding-policies/#verify-registration","title":"Verify Registration","text":"<p>After adding a built-in policy, verify the registry picks it up:</p> <pre><code>go test ./tests/internal/policy/... -v -run TestLoadBuiltinRegistry\n</code></pre> <p>The dynamic registry automatically detects new <code>.rego</code> files and updates policy counts.</p>"},{"location":"development/adding-policies/#multi-rule-policies","title":"Multi-Rule Policies","text":"<p>A single policy ID can have multiple <code>deny</code> rules (e.g., VPC-001 checks both ingress and egress). The registry deduplicates by policy ID \u2014 the policy is only counted once in compliance totals.</p> <pre><code># Rule 1: Check ingress\ndeny[result] {\n    # ...\n    result := { \"policy_id\": \"VPC-001\", ... }\n}\n\n# Rule 2: Check egress\ndeny[result] {\n    # ...\n    result := { \"policy_id\": \"VPC-001\", ... }\n}\n</code></pre>"},{"location":"development/adding-services/","title":"Adding a New AWS Service","text":"<p>This guide walks through adding support for a new AWS service to Cloudrift. Each service requires 5 new files.</p> <p>Currently supported services</p> <p>Cloudrift ships with drift detection for S3, EC2, and IAM. Use this guide to add additional services.</p>"},{"location":"development/adding-services/#checklist","title":"Checklist","text":"<ul> <li>[ ] Create the data model (<code>internal/models/</code>)</li> <li>[ ] Create the plan parser (<code>internal/parser/</code>)</li> <li>[ ] Create the AWS client (<code>internal/aws/</code>)</li> <li>[ ] Create the drift detector (<code>internal/detector/</code>)</li> <li>[ ] Create the console printer (<code>internal/detector/</code>)</li> <li>[ ] Register in <code>cmd/scan.go</code></li> <li>[ ] Add tests (<code>tests/internal/</code>)</li> </ul>"},{"location":"development/adding-services/#step-1-data-model","title":"Step 1: Data Model","text":"<p>Create <code>internal/models/&lt;service&gt;.go</code>:</p> <pre><code>package models\n\ntype RDSInstance struct {\n    Id                string\n    Name              string\n    Engine            string\n    EngineVersion     string\n    InstanceClass     string\n    StorageEncrypted  bool\n    PubliclyAccessible bool\n    MultiAZ           bool\n    Tags              map[string]string\n}\n</code></pre>"},{"location":"development/adding-services/#step-2-plan-parser","title":"Step 2: Plan Parser","text":"<p>Create <code>internal/parser/&lt;service&gt;.go</code>:</p> <pre><code>package parser\n\nimport \"github.com/inayathulla/cloudrift/internal/models\"\n\nfunc ParseRDSPlan(planPath string) ([]models.RDSInstance, error) {\n    // Read plan JSON\n    // Extract resource_changes where type == \"aws_db_instance\"\n    // Map change.after to RDSInstance structs\n    return instances, nil\n}\n</code></pre> <p>Register a loader function in <code>internal/common/bootstrap.go</code>.</p>"},{"location":"development/adding-services/#step-3-aws-client","title":"Step 3: AWS Client","text":"<p>Create <code>internal/aws/&lt;service&gt;.go</code>:</p> <pre><code>package aws\n\nimport (\n    \"context\"\n    \"github.com/aws/aws-sdk-go-v2/aws\"\n    \"github.com/aws/aws-sdk-go-v2/service/rds\"\n    \"github.com/inayathulla/cloudrift/internal/models\"\n)\n\nfunc FetchRDSInstances(cfg aws.Config) ([]models.RDSInstance, error) {\n    client := rds.NewFromConfig(cfg)\n    // Call DescribeDBInstances with pagination\n    // Map to []models.RDSInstance\n    return instances, nil\n}\n</code></pre> <p>Parallel fetching</p> <p>Use <code>errgroup.WithContext</code> for attributes that require separate API calls, following the S3 pattern in <code>internal/aws/s3.go</code>.</p>"},{"location":"development/adding-services/#step-4-drift-detector","title":"Step 4: Drift Detector","text":"<p>Create <code>internal/detector/&lt;service&gt;.go</code>:</p> <pre><code>package detector\n\ntype RDSDriftDetector struct {\n    cfg aws.Config\n}\n\nfunc NewRDSDriftDetector(cfg aws.Config) *RDSDriftDetector {\n    return &amp;RDSDriftDetector{cfg: cfg}\n}\n\nfunc (d *RDSDriftDetector) FetchLiveState() (interface{}, error) {\n    return aws.FetchRDSInstances(d.cfg)\n}\n\nfunc (d *RDSDriftDetector) DetectDrift(plan, live interface{}) ([]DriftResult, error) {\n    // Compare planned vs live attributes\n    // Return DriftResult for each resource\n}\n</code></pre>"},{"location":"development/adding-services/#step-5-console-printer","title":"Step 5: Console Printer","text":"<p>Create <code>internal/detector/&lt;service&gt;_printer.go</code>:</p> <pre><code>package detector\n\ntype RDSDriftResultPrinter struct{}\n\nfunc (p RDSDriftResultPrinter) PrintDrift(results []DriftResult, plan, live interface{}) {\n    // Colorized console output for RDS drift\n}\n</code></pre>"},{"location":"development/adding-services/#step-6-register-in-scango","title":"Step 6: Register in scan.go","text":"<p>Add a new case in the <code>switch service</code> block in <code>cmd/scan.go</code>:</p> <pre><code>case \"rds\":\n    det = detector.NewRDSDriftDetector(cfg)\n    printer = detector.RDSDriftResultPrinter{}\n    serviceName = \"RDS\"\n    pr, err := common.LoadRDSPlan(planPath)\n    if err != nil {\n        // handle error\n    }\n    planResources = pr\n    planCount = len(pr)\n</code></pre>"},{"location":"development/adding-services/#step-7-add-tests","title":"Step 7: Add Tests","text":"<p>Create tests in <code>tests/internal/</code>:</p> <ul> <li><code>tests/internal/detector/&lt;service&gt;_test.go</code> \u2014 Drift detection tests</li> <li><code>tests/internal/models/&lt;service&gt;_test.go</code> \u2014 Model tests</li> <li><code>tests/internal/parser/&lt;service&gt;_test.go</code> \u2014 Parser tests</li> </ul> <p>Follow existing test patterns using <code>testify</code> assertions.</p>"},{"location":"development/local-setup/","title":"Local Setup","text":""},{"location":"development/local-setup/#prerequisites","title":"Prerequisites","text":"Tool Version Install Go 1.24+ go.dev/dl or <code>brew install go</code> AWS CLI v2 aws.amazon.com/cli Terraform 1.0+ terraform.io (for generating plan files) Git 2.0+ <code>brew install git</code>"},{"location":"development/local-setup/#clone-and-build","title":"Clone and Build","text":"<pre><code>git clone https://github.com/inayathulla/cloudrift.git\ncd cloudrift\ngo build -o cloudrift main.go\n</code></pre>"},{"location":"development/local-setup/#install-globally","title":"Install Globally","text":"<pre><code>go install .\n</code></pre> <p>This places the <code>cloudrift</code> binary in <code>$GOPATH/bin</code>.</p>"},{"location":"development/local-setup/#running-tests","title":"Running Tests","text":"<pre><code># All tests\ngo test ./...\n\n# Verbose output\ngo test -v ./...\n\n# Specific package\ngo test ./tests/internal/detector/...\ngo test ./tests/internal/policy/...\n\n# With count (bypass cache)\ngo test -count=1 ./...\n</code></pre>"},{"location":"development/local-setup/#code-formatting","title":"Code Formatting","text":"<pre><code>go fmt ./...\n</code></pre>"},{"location":"development/local-setup/#project-layout","title":"Project Layout","text":"<pre><code># View the project structure\nls -la internal/\n</code></pre> <p>See Project Structure for the full directory layout.</p>"},{"location":"development/local-setup/#ide-setup","title":"IDE Setup","text":""},{"location":"development/local-setup/#vs-code","title":"VS Code","text":"<p>Recommended extensions:</p> <ul> <li>Go (<code>golang.go</code>) \u2014 IntelliSense, debugging, formatting</li> <li>OPA (<code>tsandall.opa</code>) \u2014 Rego syntax highlighting</li> </ul>"},{"location":"development/local-setup/#goland-intellij","title":"GoLand / IntelliJ","text":"<p>Go support is built-in. Enable the OPA plugin for <code>.rego</code> file support.</p>"},{"location":"development/local-setup/#running-a-local-scan","title":"Running a Local Scan","text":"<pre><code># 1. Ensure AWS credentials are configured\naws sts get-caller-identity\n\n# 2. Generate a Terraform plan\ncd /path/to/terraform/project\nterraform plan -out=tfplan\nterraform show -json tfplan &gt; plan.json\n\n# 3. Create a config\ncat &gt; cloudrift-s3.yml &lt;&lt;EOF\naws_profile: default\nregion: us-east-1\nplan_path: /path/to/plan.json\nEOF\n\n# 4. Run the scan\n./cloudrift scan --service=s3\n</code></pre>"},{"location":"development/testing/","title":"Testing","text":"<p>Cloudrift uses Go's built-in testing framework with testify for assertions.</p>"},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<p>Tests live in <code>tests/internal/</code>, mirroring the <code>internal/</code> package structure:</p> <pre><code>tests/\n\u2514\u2500\u2500 internal/\n    \u251c\u2500\u2500 detector/\n    \u2502   \u251c\u2500\u2500 s3_test.go          # S3 drift detection scenarios\n    \u2502   \u251c\u2500\u2500 ec2_test.go         # EC2 drift detection scenarios\n    \u2502   \u2514\u2500\u2500 registry_test.go    # Service registry tests\n    \u251c\u2500\u2500 models/\n    \u2502   \u2514\u2500\u2500 s3bucket_test.go    # Model tests\n    \u251c\u2500\u2500 output/\n    \u2502   \u2514\u2500\u2500 formatter_test.go   # JSON, SARIF, Console formatter tests\n    \u251c\u2500\u2500 parser/\n    \u2502   \u2514\u2500\u2500 parser_test.go      # Plan parser tests\n    \u2514\u2500\u2500 policy/\n        \u251c\u2500\u2500 policy_test.go      # Policy engine evaluation tests\n        \u2514\u2500\u2500 registry_test.go    # Policy registry + framework filtering tests\n</code></pre>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":"<pre><code># All tests\ngo test ./...\n\n# Verbose output\ngo test -v ./...\n\n# Specific package\ngo test ./tests/internal/detector/...\n\n# Specific test function\ngo test -v ./tests/internal/policy/... -run TestPolicyRegistry_FilterByFrameworks\n\n# Bypass test cache\ngo test -count=1 ./...\n</code></pre>"},{"location":"development/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"development/testing/#basic-test-pattern","title":"Basic Test Pattern","text":"<pre><code>package detector\n\nimport (\n    \"testing\"\n\n    \"github.com/inayathulla/cloudrift/internal/detector\"\n    \"github.com/stretchr/testify/assert\"\n    \"github.com/stretchr/testify/require\"\n)\n\nfunc TestS3DriftDetector_NoDrift(t *testing.T) {\n    planned := models.S3Bucket{\n        Name:              \"my-bucket\",\n        VersioningEnabled: true,\n    }\n    live := models.S3Bucket{\n        Name:              \"my-bucket\",\n        VersioningEnabled: true,\n    }\n\n    det := detector.NewS3DriftDetector(cfg)\n    results, err := det.DetectDrift(\n        []models.S3Bucket{planned},\n        []models.S3Bucket{live},\n    )\n\n    require.NoError(t, err)\n    assert.Empty(t, results[0].TagDiffs)\n    assert.False(t, results[0].VersioningDiff)\n}\n</code></pre>"},{"location":"development/testing/#table-driven-tests","title":"Table-Driven Tests","text":"<pre><code>func TestSeverityLevels(t *testing.T) {\n    tests := []struct {\n        name     string\n        severity string\n        expected string\n    }{\n        {\"critical maps to error\", \"critical\", \"error\"},\n        {\"warning maps to warning\", \"warning\", \"warning\"},\n        {\"info maps to note\", \"info\", \"note\"},\n    }\n\n    for _, tc := range tests {\n        t.Run(tc.name, func(t *testing.T) {\n            result := mapSeverity(tc.severity)\n            assert.Equal(t, tc.expected, result)\n        })\n    }\n}\n</code></pre>"},{"location":"development/testing/#key-test-files","title":"Key Test Files","text":""},{"location":"development/testing/#drift-detection-tests","title":"Drift Detection Tests","text":"<p><code>tests/internal/detector/s3_test.go</code> covers:</p> <ul> <li>No drift (identical plan and live)</li> <li>ACL drift</li> <li>Tag drift (modified, extra, missing tags)</li> <li>Versioning drift</li> <li>Encryption drift</li> <li>Logging drift</li> <li>Public access block drift</li> <li>Lifecycle rule drift</li> <li>Missing bucket (in plan but not in AWS)</li> </ul>"},{"location":"development/testing/#policy-registry-tests","title":"Policy Registry Tests","text":"<p><code>tests/internal/policy/registry_test.go</code> covers:</p> <ul> <li>Registry loads non-empty</li> <li>Total matches policy map length</li> <li>Category totals are consistent</li> <li>Expected categories exist (security, tagging, cost)</li> <li>Expected frameworks exist (5 frameworks)</li> <li>Known policies exist with correct categories</li> <li>Framework mappings are correct</li> <li>No duplicate policy IDs</li> <li>Framework filtering (single, multiple, empty)</li> <li>Filtering doesn't mutate the original</li> <li>Filtered category totals are consistent</li> <li>KnownFrameworks returns sorted list</li> <li>Idempotent loading</li> </ul>"},{"location":"development/testing/#formatter-tests","title":"Formatter Tests","text":"<p><code>tests/internal/output/formatter_test.go</code> covers:</p> <ul> <li>JSON format validity and field presence</li> <li>SARIF schema compliance</li> <li>Console output for drift/no-drift</li> <li>Compliance data in JSON output</li> <li>Framework filter metadata (<code>active_frameworks</code>)</li> <li>Backward compatibility (no compliance when nil)</li> <li>Severity mapping</li> </ul>"},{"location":"development/testing/#ci-test-workflow","title":"CI Test Workflow","text":"<p>Tests run automatically on push and PR via GitHub Actions:</p> <pre><code>- name: Run Tests\n  run: go test -v ./... -json &gt; test-results/report.json\n</code></pre> <p>See CI/CD Integration for the full workflow.</p>"},{"location":"features/ci-cd/","title":"CI/CD Integration","text":"<p>Cloudrift integrates into CI/CD pipelines to catch drift and policy violations before deployment.</p>"},{"location":"features/ci-cd/#github-actions","title":"GitHub Actions","text":""},{"location":"features/ci-cd/#basic-workflow","title":"Basic Workflow","text":"<pre><code>name: Cloudrift Scan\non:\n  pull_request:\n    branches: [main]\n\njobs:\n  scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup Go\n        uses: actions/setup-go@v5\n        with:\n          go-version: '1.24'\n\n      - name: Install Cloudrift\n        run: go install github.com/inayathulla/cloudrift@latest\n\n      - name: Generate Terraform Plan\n        run: |\n          terraform init\n          terraform plan -out=tfplan\n          terraform show -json tfplan &gt; plan.json\n\n      - name: Run Cloudrift Scan\n        run: cloudrift scan --service=s3 --fail-on-violation --no-emoji\n        env:\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          AWS_REGION: us-east-1\n</code></pre>"},{"location":"features/ci-cd/#with-sarif-upload","title":"With SARIF Upload","text":"<p>Upload results to GitHub's Security tab:</p> <pre><code>name: Cloudrift Security Scan\non:\n  push:\n    branches: [main]\n  pull_request:\n\npermissions:\n  security-events: write\n\njobs:\n  scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup Go\n        uses: actions/setup-go@v5\n        with:\n          go-version: '1.24'\n\n      - name: Install Cloudrift\n        run: go install github.com/inayathulla/cloudrift@latest\n\n      - name: Generate Terraform Plan\n        run: |\n          terraform init\n          terraform plan -out=tfplan\n          terraform show -json tfplan &gt; plan.json\n\n      - name: Run Cloudrift Scan\n        run: |\n          cloudrift scan --service=s3 \\\n            --format=sarif --output=results.sarif \\\n            --no-emoji\n        continue-on-error: true\n        env:\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n\n      - name: Upload SARIF\n        uses: github/codeql-action/upload-sarif@v3\n        with:\n          sarif_file: results.sarif\n</code></pre>"},{"location":"features/ci-cd/#framework-specific-scan","title":"Framework-Specific Scan","text":"<pre><code>      - name: HIPAA Compliance Check\n        run: |\n          cloudrift scan --service=s3 \\\n            --frameworks=hipaa \\\n            --fail-on-violation \\\n            --no-emoji\n</code></pre>"},{"location":"features/ci-cd/#docker-based-workflow","title":"Docker-Based Workflow","text":"<pre><code>      - name: Run Cloudrift via Docker\n        run: |\n          docker run --rm \\\n            -e AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }} \\\n            -e AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }} \\\n            -v $(pwd):/work \\\n            inayathulla/cloudrift:latest scan \\\n            --config=/work/cloudrift-s3.yml \\\n            --service=s3 \\\n            --fail-on-violation \\\n            --no-emoji\n</code></pre>"},{"location":"features/ci-cd/#gitlab-ci","title":"GitLab CI","text":"<pre><code>cloudrift-scan:\n  stage: test\n  image: golang:1.24\n  before_script:\n    - go install github.com/inayathulla/cloudrift@latest\n  script:\n    - terraform init\n    - terraform plan -out=tfplan\n    - terraform show -json tfplan &gt; plan.json\n    - cloudrift scan --service=s3 --fail-on-violation --no-emoji\n  variables:\n    AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID\n    AWS_SECRET_ACCESS_KEY: $AWS_SECRET_ACCESS_KEY\n    AWS_REGION: us-east-1\n  only:\n    - merge_requests\n</code></pre>"},{"location":"features/ci-cd/#pipeline-gating","title":"Pipeline Gating","text":"<p>Use <code>--fail-on-violation</code> to gate deployments:</p> Exit Code Meaning Pipeline Result <code>0</code> No violations  Pass <code>1</code> Scan error  Fail <code>2</code> Violations found  Fail <p>Gradual adoption</p> <p>Start without <code>--fail-on-violation</code> to see results without breaking builds. Enable it once your team has addressed existing violations.</p>"},{"location":"features/ci-cd/#json-reports-as-artifacts","title":"JSON Reports as Artifacts","text":"<p>Save scan results as CI artifacts for auditing:</p> <pre><code>      - name: Run Scan\n        run: |\n          cloudrift scan --service=s3 \\\n            --format=json --output=cloudrift-report.json \\\n            --no-emoji\n\n      - name: Upload Report\n        uses: actions/upload-artifact@v4\n        with:\n          name: cloudrift-report\n          path: cloudrift-report.json\n</code></pre>"},{"location":"features/compliance/","title":"Compliance Frameworks","text":"<p>Cloudrift maps its 49 built-in policies to 5 industry compliance frameworks, providing automated compliance scoring. Policy counts are computed dynamically from the <code>.rego</code> files \u2014 never hardcoded.</p>"},{"location":"features/compliance/#frameworks","title":"Frameworks","text":"Framework Policies Description SOC 2 Type II 40 Trust services criteria \u2014 security, availability, confidentiality ISO 27001 39 Information security management system controls PCI DSS 34 Payment card industry data security standard HIPAA 26 Health data privacy and security rules GDPR 18 EU data protection and privacy regulation"},{"location":"features/compliance/#policy-categories","title":"Policy Categories","text":"Category Policies Description Security 42 Encryption, access control, network, IAM, audit logging Tagging 4 Resource tagging for cost allocation and governance Cost 3 Instance sizing and generation optimization"},{"location":"features/compliance/#scoring-calculation","title":"Scoring Calculation","text":"<p>Each framework's compliance score is calculated as:</p> <pre><code>Score = (Passing Policies / Total Mapped Policies) x 100%\n</code></pre> <p>A policy passes if zero violations are found for it across all scanned resources. A single violation causes the policy to fail.</p>"},{"location":"features/compliance/#score-thresholds","title":"Score Thresholds","text":"Score Color Rating 100%  Green Full compliance 80-99%  Yellow Needs attention &lt; 80%  Red Critical gaps"},{"location":"features/compliance/#framework-filtering","title":"Framework Filtering","text":"<p>Use <code>--frameworks</code> to scope evaluation to specific frameworks:</p> <pre><code># HIPAA-only\ncloudrift scan --service=s3 --frameworks=hipaa\n\n# Multiple frameworks\ncloudrift scan --service=s3 --frameworks=hipaa,gdpr\n\n# With JSON output\ncloudrift scan --service=s3 --format=json --frameworks=soc2,pci_dss\n</code></pre> <p>When <code>--frameworks</code> is set:</p> <ul> <li>Only violations from policies mapped to selected frameworks are shown</li> <li>Compliance scoring uses filtered totals</li> <li>Console output shows the active filter in the header</li> <li>JSON output includes <code>active_frameworks</code> array</li> </ul>"},{"location":"features/compliance/#console-output-with-filter","title":"Console Output with Filter","text":"<pre><code>\ud83d\udd10 Filtering by frameworks: hipaa, gdpr\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n      COMPLIANCE SUMMARY (HIPAA, GDPR)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n</code></pre>"},{"location":"features/compliance/#json-output-with-filter","title":"JSON Output with Filter","text":"<pre><code>{\n  \"compliance\": {\n    \"overall_percentage\": 96.43,\n    \"total_policies\": 28,\n    \"active_frameworks\": [\"hipaa\", \"gdpr\"]\n  }\n}\n</code></pre>"},{"location":"features/compliance/#framework-validation","title":"Framework Validation","text":"<p>Unknown framework names are rejected with a list of valid options:</p> <pre><code>\u274c Unknown framework: hipa\n  Available frameworks: gdpr, hipaa, iso_27001, pci_dss, soc2\n</code></pre>"},{"location":"features/compliance/#framework-details","title":"Framework Details","text":""},{"location":"features/compliance/#hipaa-26-policies","title":"HIPAA (26 policies)","text":"<p>Health Insurance Portability and Accountability Act \u2014 protects sensitive patient health information.</p> <p>Focus areas: Encryption at rest, encryption in transit, access controls, audit logging, network isolation.</p> <p>Key policies: S3-001 (encryption), EC2-002 (root volume encryption), RDS-001/RDS-002 (database security), IAM-001 (least privilege), CT-001 (audit logging).</p>"},{"location":"features/compliance/#gdpr-18-policies","title":"GDPR (18 policies)","text":"<p>General Data Protection Regulation \u2014 EU regulation on data privacy and protection.</p> <p>Focus areas: Data encryption, access controls, audit trails, data protection by design.</p> <p>Key policies: S3-001 through S3-008 (data storage security), EBS-001/EBS-002 (volume encryption), LOG-001/LOG-002 (audit logging).</p>"},{"location":"features/compliance/#iso-27001-39-policies","title":"ISO 27001 (39 policies)","text":"<p>International standard for information security management systems.</p> <p>Focus areas: Comprehensive security controls, risk management, access management, operations security.</p> <p>Key policies: Broadest coverage across all AWS services \u2014 encryption, access control, network security, key management, and audit logging.</p>"},{"location":"features/compliance/#pci-dss-34-policies","title":"PCI DSS (34 policies)","text":"<p>Payment Card Industry Data Security Standard \u2014 protects cardholder data.</p> <p>Focus areas: Network security, strong access control, encryption, monitoring, vulnerability management.</p> <p>Key policies: SG-001 through SG-004 (network security), IAM-001 through IAM-003 (access control), CT-001 through CT-003 (monitoring).</p>"},{"location":"features/compliance/#soc-2-type-ii-40-policies","title":"SOC 2 Type II (40 policies)","text":"<p>Service Organization Control 2 \u2014 covers security, availability, processing integrity, confidentiality, and privacy.</p> <p>Focus areas: The most comprehensive framework, covering all aspects of security controls.</p> <p>Key policies: Nearly all 49 policies \u2014 the broadest compliance framework supported.</p>"},{"location":"features/drift-detection/","title":"Drift Detection","text":"<p>Cloudrift detects configuration drift by comparing your Terraform plan JSON against the actual state of AWS resources. Unlike post-apply tools like driftctl, Cloudrift catches drift before <code>terraform apply</code>.</p>"},{"location":"features/drift-detection/#how-it-works","title":"How It Works","text":"<pre><code>graph LR\n    A[\"Terraform Plan&lt;br/&gt;(JSON)\"] --&gt; C[\"Cloudrift\"]\n    B[\"Live AWS State&lt;br/&gt;(API calls)\"] --&gt; C\n    C --&gt; D[\"Drift Report\"]</code></pre> <ol> <li>Parse \u2014 Cloudrift reads <code>resource_changes[].change.after</code> from your Terraform plan JSON</li> <li>Fetch \u2014 Queries AWS APIs for the current state of each resource</li> <li>Compare \u2014 Attribute-by-attribute comparison between planned and live state</li> <li>Report \u2014 Outputs differences with severity levels</li> </ol>"},{"location":"features/drift-detection/#supported-services","title":"Supported Services","text":""},{"location":"features/drift-detection/#s3-buckets","title":"S3 Buckets","text":"Attribute Description ACL Bucket access control list (private, public-read, etc.) Tags Resource tags (key-value pairs) Versioning Whether versioning is enabled Encryption Server-side encryption algorithm (AES256, aws:kms) Logging Access logging configuration Public Access Block Block public ACLs, policies, and bucket access Lifecycle Rules Object lifecycle management rules <p>S3 attributes are fetched in parallel using Go's <code>errgroup</code> \u2014 all 7 API calls per bucket run concurrently.</p>"},{"location":"features/drift-detection/#ec2-instances","title":"EC2 Instances","text":"Attribute Description Instance Type Instance size (t3.micro, m5.large, etc.) AMI Amazon Machine Image ID Subnet VPC subnet placement Security Groups Attached security group IDs Tags Resource tags EBS Optimization Whether EBS-optimized storage is enabled Monitoring Detailed monitoring status <p>EC2 instances are fetched with pagination to support large fleets.</p>"},{"location":"features/drift-detection/#iam-resources","title":"IAM Resources","text":"Attribute Description Roles \u2014 Trust Policy AssumeRolePolicyDocument (JSON-normalized comparison) Roles \u2014 Max Session Maximum session duration in seconds Roles \u2014 Description Role description text Roles \u2014 Path IAM path for the role Roles \u2014 Attached Policies Managed policy ARNs attached to the role Users \u2014 Path IAM path for the user Users \u2014 Attached Policies Managed policy ARNs attached to the user Policies \u2014 Document Policy document JSON (JSON-normalized comparison) Policies \u2014 Description Policy description text Policies \u2014 Path IAM path for the policy Groups \u2014 Path IAM path for the group Groups \u2014 Attached Policies Managed policy ARNs attached to the group Groups \u2014 Members Group membership (user names) All \u2014 Tags Resource tags (key-value pairs) <p>IAM resources (roles, users, policies, groups) are fetched in parallel using <code>errgroup</code>. AWS service-linked roles and AWS-managed policies are excluded to focus on customer-managed resources. Policy documents and trust policies are compared using JSON normalization to avoid false positives from formatting differences.</p>"},{"location":"features/drift-detection/#drift-types","title":"Drift Types","text":""},{"location":"features/drift-detection/#missing-resources","title":"Missing Resources","text":"<p>A resource exists in the Terraform plan but not in AWS. This is flagged as critical severity.</p> <pre><code>\u274c MISSING: S3 bucket \"my-bucket\" exists in plan but not in AWS\n</code></pre>"},{"location":"features/drift-detection/#attribute-differences","title":"Attribute Differences","text":"<p>A resource exists in both plan and AWS but has different attribute values.</p> <pre><code>\u26a0\ufe0f  DRIFT: my-bucket\n  versioning_enabled: true \u2192 false\n  encryption_algorithm: aws:kms \u2192 AES256\n</code></pre>"},{"location":"features/drift-detection/#extra-tags","title":"Extra Tags","text":"<p>Tags exist in AWS but are not defined in the Terraform plan.</p> <pre><code>\u2139\ufe0f  EXTRA TAGS: my-bucket\n  ManagedBy: manual (not in plan)\n</code></pre>"},{"location":"features/drift-detection/#pre-apply-vs-post-apply","title":"Pre-Apply vs Post-Apply","text":"Aspect Cloudrift (Pre-Apply) driftctl (Post-Apply) When Before <code>terraform apply</code> After <code>terraform apply</code> Input Terraform plan JSON Terraform state file Catches Config someone changed manually State file vs live Use case CI/CD gate, code review Ongoing monitoring Risk None \u2014 read-only None \u2014 read-only <p>Complementary tools</p> <p>Cloudrift and post-apply tools serve different purposes. Use Cloudrift in CI/CD to catch drift before deploying, and a post-apply tool for ongoing monitoring.</p>"},{"location":"features/policy-engine/","title":"Policy Engine","text":"<p>Cloudrift includes a built-in policy engine powered by Open Policy Agent (OPA). It evaluates 49 security, tagging, and cost policies against your Terraform resources.</p>"},{"location":"features/policy-engine/#how-it-works","title":"How It Works","text":"<pre><code>graph LR\n    A[\"Terraform Resources\"] --&gt; B[\"Policy Engine&lt;br/&gt;(OPA)\"]\n    C[\"49 Built-in Policies&lt;br/&gt;(.rego files)\"] --&gt; B\n    D[\"Custom Policies&lt;br/&gt;(--policy-dir)\"] --&gt; B\n    B --&gt; E[\"Violations\"]\n    B --&gt; F[\"Warnings\"]\n    B --&gt; G[\"Compliance Scores\"]</code></pre> <ol> <li>Resources from the Terraform plan are converted to policy inputs</li> <li>The OPA compiler evaluates all <code>.rego</code> policies against each resource</li> <li><code>deny</code> rules produce violations (blocking findings)</li> <li><code>warn</code> rules produce warnings (advisory findings)</li> <li>Compliance scores are computed from pass/fail counts</li> </ol>"},{"location":"features/policy-engine/#built-in-policies","title":"Built-in Policies","text":"<p>Cloudrift ships with 49 policies embedded in the binary:</p> Category Policies Description Security 42 Encryption, access control, network, IAM, audit logging Tagging 4 Resource tagging for governance and cost allocation Cost 3 Instance sizing and generation optimization <p>See the Policy Reference for the full list.</p>"},{"location":"features/policy-engine/#custom-policies","title":"Custom Policies","text":"<p>Add your own OPA policies with <code>--policy-dir</code>:</p> <pre><code>cloudrift scan --service=s3 --policy-dir=./my-policies\n</code></pre> <p>Custom policies are loaded alongside built-in policies. Place <code>.rego</code> files in the specified directory.</p>"},{"location":"features/policy-engine/#writing-a-custom-policy","title":"Writing a Custom Policy","text":"<pre><code>package cloudrift.custom\n\ndeny[result] {\n    input.resource.type == \"aws_s3_bucket\"\n    not input.resource.planned.tags.Team\n\n    result := {\n        \"policy_id\": \"CUSTOM-001\",\n        \"policy_name\": \"Team Tag Required\",\n        \"msg\": sprintf(\"S3 bucket '%s' is missing required 'Team' tag\", [input.resource.address]),\n        \"severity\": \"medium\",\n        \"remediation\": \"Add tags = { Team = \\\"your-team\\\" } to the resource\",\n        \"category\": \"tagging\",\n        \"frameworks\": [\"soc2\"],\n    }\n}\n</code></pre>"},{"location":"features/policy-engine/#policy-structure","title":"Policy Structure","text":"<p>Every policy result must include:</p> Field Type Required Description <code>policy_id</code> string yes Unique identifier (e.g., \"CUSTOM-001\") <code>policy_name</code> string yes Human-readable name <code>msg</code> string yes Violation description <code>severity</code> string yes <code>critical</code>, <code>high</code>, <code>medium</code>, <code>low</code>, <code>info</code> <code>remediation</code> string no Fix guidance <code>category</code> string no <code>security</code>, <code>tagging</code>, <code>cost</code> <code>frameworks</code> array no Compliance frameworks (<code>hipaa</code>, <code>gdpr</code>, <code>iso_27001</code>, <code>pci_dss</code>, <code>soc2</code>)"},{"location":"features/policy-engine/#policy-input","title":"Policy Input","text":"<p>Each resource is provided as an OPA input with this structure:</p> <pre><code>{\n  \"resource\": {\n    \"type\": \"aws_s3_bucket\",\n    \"address\": \"aws_s3_bucket.my_bucket\",\n    \"planned\": {\n      \"bucket\": \"my-bucket\",\n      \"acl\": \"private\",\n      \"tags\": { \"Environment\": \"prod\" },\n      \"versioning_enabled\": true,\n      \"encryption_algorithm\": \"AES256\",\n      \"logging_enabled\": false,\n      \"public_access_block\": {\n        \"block_public_acls\": true,\n        \"block_public_policy\": true,\n        \"ignore_public_acls\": true,\n        \"restrict_public_buckets\": true\n      }\n    },\n    \"drift\": {\n      \"has_drift\": false,\n      \"missing\": false\n    }\n  }\n}\n</code></pre>"},{"location":"features/policy-engine/#skipping-policies","title":"Skipping Policies","text":"<p>To run drift detection without policy evaluation:</p> <pre><code>cloudrift scan --service=s3 --skip-policies\n</code></pre>"},{"location":"features/policy-engine/#policy-loading","title":"Policy Loading","text":"<p>Policies are embedded in the Cloudrift binary using Go's <code>//go:embed</code> directive. At runtime:</p> <ol> <li>Embedded <code>.rego</code> files are extracted to a temporary directory</li> <li>OPA compiler parses and compiles all modules together</li> <li>The temporary directory is cleaned up after compilation</li> <li>Compiled modules remain in memory for evaluation</li> </ol> <p>Custom policies (from <code>--policy-dir</code>) are loaded and compiled alongside built-in policies.</p>"},{"location":"getting-started/aws-credentials/","title":"AWS Credentials","text":"<p>Cloudrift requires read-only access to AWS resources for drift detection.</p>"},{"location":"getting-started/aws-credentials/#required-iam-permissions","title":"Required IAM Permissions","text":""},{"location":"getting-started/aws-credentials/#minimum-policy","title":"Minimum Policy","text":"<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:ListAllMyBuckets\",\n        \"s3:GetBucketLocation\",\n        \"s3:GetBucketVersioning\",\n        \"s3:GetBucketEncryption\",\n        \"s3:GetBucketLogging\",\n        \"s3:GetBucketTagging\",\n        \"s3:GetBucketPublicAccessBlock\",\n        \"s3:GetLifecycleConfiguration\",\n        \"s3:GetBucketAcl\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeTags\",\n        \"sts:GetCallerIdentity\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre>"},{"location":"getting-started/aws-credentials/#per-service-breakdown","title":"Per-Service Breakdown","text":"S3EC2Common Permission Purpose <code>s3:ListAllMyBuckets</code> Enumerate buckets <code>s3:GetBucketLocation</code> Determine bucket region <code>s3:GetBucketVersioning</code> Check versioning status <code>s3:GetBucketEncryption</code> Check encryption config <code>s3:GetBucketLogging</code> Check access logging <code>s3:GetBucketTagging</code> Read resource tags <code>s3:GetBucketPublicAccessBlock</code> Check public access settings <code>s3:GetLifecycleConfiguration</code> Read lifecycle rules <code>s3:GetBucketAcl</code> Read bucket ACL Permission Purpose <code>ec2:DescribeInstances</code> List and describe EC2 instances <code>ec2:DescribeTags</code> Read instance tags Permission Purpose <code>sts:GetCallerIdentity</code> Validate credentials and display account info"},{"location":"getting-started/aws-credentials/#aws-profile-configuration","title":"AWS Profile Configuration","text":"<p>Configure profiles in <code>~/.aws/credentials</code>:</p> <pre><code>[default]\naws_access_key_id = AKIAIOSFODNN7EXAMPLE\naws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\n[production]\naws_access_key_id = AKIAI44QH8DHBEXAMPLE\naws_secret_access_key = je7MtGbClwBF/2Zp9Utk/h3yCo8nvbEXAMPLEKEY\n</code></pre> <p>Reference the profile in your config:</p> <pre><code>aws_profile: production\nregion: us-east-1\nplan_path: ./plan.json\n</code></pre>"},{"location":"getting-started/aws-credentials/#environment-variables","title":"Environment Variables","text":"<p>AWS SDK v2 supports standard environment variables:</p> Variable Description <code>AWS_PROFILE</code> Profile name from credentials file <code>AWS_REGION</code> AWS region <code>AWS_ACCESS_KEY_ID</code> Access key (overrides profile) <code>AWS_SECRET_ACCESS_KEY</code> Secret key (overrides profile) <code>AWS_SESSION_TOKEN</code> Session token for temporary credentials"},{"location":"getting-started/aws-credentials/#iam-roles-assumed-roles","title":"IAM Roles (Assumed Roles)","text":"<p>For cross-account scanning, configure a role in <code>~/.aws/config</code>:</p> <pre><code>[profile cross-account]\nrole_arn = arn:aws:iam::ACCOUNT_ID:role/CloudriftReadOnly\nsource_profile = default\nregion = us-east-1\n</code></pre> <p>Then reference it:</p> <pre><code>aws_profile: cross-account\n</code></pre> <p>Permissions</p> <p>Cloudrift only needs read-only access. Never grant write or delete permissions to the scanning role.</p>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>Cloudrift uses a YAML configuration file to define AWS credentials, region, and scan parameters.</p>"},{"location":"getting-started/configuration/#config-file-format","title":"Config File Format","text":"<pre><code>aws_profile: default\nregion: us-east-1\nplan_path: ./plan.json\n</code></pre>"},{"location":"getting-started/configuration/#fields","title":"Fields","text":"Field Type Required Default Description <code>aws_profile</code> string yes <code>default</code> AWS credentials profile name from <code>~/.aws/credentials</code> <code>region</code> string yes <code>us-east-1</code> AWS region to scan <code>plan_path</code> string yes \u2014 Path to Terraform plan JSON file"},{"location":"getting-started/configuration/#service-specific-configs","title":"Service-Specific Configs","text":"<p>Each AWS service needs its own Terraform plan file. Create separate configs per service:</p> S3 (cloudrift-s3.yml)EC2 (cloudrift-ec2.yml)IAM (cloudrift-iam.yml) <pre><code>aws_profile: default\nregion: us-east-1\nplan_path: ./plan.json\n</code></pre> <pre><code>aws_profile: default\nregion: us-east-1\nplan_path: ./ec2-plan.json\n</code></pre> <pre><code>aws_profile: default\nregion: us-east-1\nplan_path: ./iam-plan.json\n</code></pre> <p>Use the <code>--config</code> flag to select the config:</p> <pre><code>cloudrift scan --config=cloudrift-ec2.yml --service=ec2\n</code></pre>"},{"location":"getting-started/configuration/#generating-the-plan-file","title":"Generating the Plan File","text":"<p>Cloudrift requires a Terraform plan in JSON format. Generate it with:</p> <pre><code># 1. Create the binary plan\nterraform plan -out=tfplan\n\n# 2. Convert to JSON\nterraform show -json tfplan &gt; plan.json\n</code></pre> <p>Plan file scope</p> <p>The plan file should contain all the resources you want to scan. Cloudrift extracts resources matching the selected <code>--service</code> type (e.g., <code>aws_s3_bucket</code> for S3, <code>aws_instance</code> for EC2, <code>aws_iam_role</code>/<code>aws_iam_user</code>/<code>aws_iam_policy</code>/<code>aws_iam_group</code> for IAM).</p>"},{"location":"getting-started/configuration/#plan-file-structure","title":"Plan File Structure","text":"<p>Cloudrift reads resources from the <code>resource_changes[].change.after</code> path in the plan JSON. Each resource change must contain the resource type, address, and planned attribute values.</p>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>AWS credentials can also be configured via environment variables:</p> <pre><code>export AWS_PROFILE=production\nexport AWS_REGION=eu-west-1\n</code></pre> <p>These are picked up by the AWS SDK automatically and override the config file values.</p>"},{"location":"getting-started/configuration/#config-file-locations","title":"Config File Locations","text":"<p>By default, Cloudrift looks for <code>cloudrift-s3.yml</code> in the current working directory. Override with:</p> <pre><code>cloudrift scan --config=/path/to/config/cloudrift-s3.yml --service=s3\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Cloudrift can be installed via Go, Docker, or built from source.</p>"},{"location":"getting-started/installation/#go-install-recommended","title":"Go Install (Recommended)","text":"<pre><code>go install github.com/inayathulla/cloudrift@latest\n</code></pre> <p>Requires Go 1.24+</p> <p>Cloudrift uses Go 1.24 features. Verify your Go version with <code>go version</code>.</p>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>cloudrift --help\n</code></pre>"},{"location":"getting-started/installation/#docker","title":"Docker","text":"<pre><code>docker pull inayathulla/cloudrift:latest\n</code></pre>"},{"location":"getting-started/installation/#run-a-scan","title":"Run a Scan","text":"<pre><code>docker run --rm \\\n  -v ~/.aws:/root/.aws:ro \\\n  -v $(pwd):/work \\\n  inayathulla/cloudrift:latest scan \\\n  --config=/work/cloudrift-s3.yml \\\n  --service=s3\n</code></pre>"},{"location":"getting-started/installation/#available-tags","title":"Available Tags","text":"Tag Description <code>latest</code> Latest stable release <code>v1.0.0</code> Specific version"},{"location":"getting-started/installation/#build-from-source","title":"Build from Source","text":"<pre><code>git clone https://github.com/inayathulla/cloudrift.git\ncd cloudrift\ngo build -o cloudrift main.go\n</code></pre> <p>Optionally install to your Go bin path:</p> <pre><code>go install .\n</code></pre>"},{"location":"getting-started/installation/#dockerfile","title":"Dockerfile","text":"<p>The included Dockerfile produces a minimal Alpine-based image:</p> <pre><code>docker build -t cloudrift .\n</code></pre> <p>The multi-stage build compiles a statically-linked binary (<code>CGO_ENABLED=0</code>) and copies it to an <code>alpine:latest</code> runtime image running as a non-root <code>cloudrift</code> user.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"Tool Version Required For Go 1.24+ Building from source or <code>go install</code> AWS CLI v2 Configuring AWS credentials Terraform 1.0+ Generating plan JSON files Docker 20+ Running the Docker image"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Configure Cloudrift with a <code>cloudrift-s3.yml</code> file</li> <li>Set up AWS credentials with the right permissions</li> <li>Run your first scan</li> </ul>"},{"location":"policies/cloudtrail/","title":"CloudTrail Policies","text":"<p>3 policies covering audit trail security.</p> ID Name Severity Frameworks CT-001 CloudTrail KMS Encryption HIGH HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2 CT-002 CloudTrail Log File Validation MEDIUM PCI DSS, ISO 27001, SOC 2 CT-003 CloudTrail Multi-Region MEDIUM HIPAA, PCI DSS, ISO 27001, SOC 2"},{"location":"policies/cloudtrail/#ct-001","title":"CT-001","text":"<p>CloudTrail KMS Encryption | HIGH</p> <p>Frameworks: HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2</p> <p>CloudTrail must be encrypted with a KMS key. Encrypting CloudTrail logs with a customer-managed KMS key provides additional protection for sensitive audit data and enables fine-grained access control over who can read the log files.</p> <p>Remediation:</p> <pre><code>resource \"aws_cloudtrail\" \"example\" {\n  name                          = \"example-trail\"\n  s3_bucket_name                = aws_s3_bucket.trail.id\n  kms_key_id                    = aws_kms_key.cloudtrail.arn\n  enable_log_file_validation    = true\n  is_multi_region_trail         = true\n}\n</code></pre> <p>Resource type: <code>aws_cloudtrail</code></p>"},{"location":"policies/cloudtrail/#ct-002","title":"CT-002","text":"<p>CloudTrail Log File Validation | MEDIUM</p> <p>Frameworks: PCI DSS, ISO 27001, SOC 2</p> <p>CloudTrail should enable log file validation to detect tampering. Log file validation creates a digitally signed digest file containing a hash of each log file, allowing you to determine whether a log file was modified or deleted after CloudTrail delivered it.</p> <p>Remediation:</p> <pre><code>resource \"aws_cloudtrail\" \"example\" {\n  name                          = \"example-trail\"\n  s3_bucket_name                = aws_s3_bucket.trail.id\n  enable_log_file_validation    = true\n}\n</code></pre> <p>Resource type: <code>aws_cloudtrail</code></p>"},{"location":"policies/cloudtrail/#ct-003","title":"CT-003","text":"<p>CloudTrail Multi-Region | MEDIUM</p> <p>Frameworks: HIPAA, PCI DSS, ISO 27001, SOC 2</p> <p>CloudTrail should be configured as multi-region trail. A multi-region trail ensures that API activity across all AWS regions is captured in a single trail, preventing gaps in audit coverage if resources are created in unexpected regions.</p> <p>Remediation:</p> <pre><code>resource \"aws_cloudtrail\" \"example\" {\n  name                          = \"example-trail\"\n  s3_bucket_name                = aws_s3_bucket.trail.id\n  is_multi_region_trail         = true\n}\n</code></pre> <p>Resource type: <code>aws_cloudtrail</code></p>"},{"location":"policies/cost/","title":"Cost Policies","text":"<p>2 policies covering instance cost optimization.</p> ID Name Severity Frameworks COST-002 Very Large Instance Size LOW -- COST-003 Previous Generation Instance LOW --"},{"location":"policies/cost/#cost-002","title":"COST-002","text":"<p>Very Large Instance Size | LOW</p> <p>Frameworks: --</p> <p>EC2 instance uses very large size (16xlarge/24xlarge) with monthly cost exceeding $5,000. Very large instances represent significant cloud spend and are often over-provisioned. Verify the workload genuinely requires this capacity, and consider auto-scaling groups to match capacity to demand.</p> <p>Remediation:</p> <pre><code>resource \"aws_instance\" \"example\" {\n  # Review whether this instance size is necessary.\n  # Consider using auto-scaling instead of a single very large instance.\n  instance_type = \"m5.4xlarge\"  # Downsize from 16xlarge/24xlarge\n\n  # Alternatively, use an Auto Scaling Group to scale horizontally\n  # resource \"aws_autoscaling_group\" \"example\" {\n  #   min_size         = 2\n  #   max_size         = 10\n  #   desired_capacity = 2\n  #   launch_template {\n  #     id      = aws_launch_template.example.id\n  #     version = \"$Latest\"\n  #   }\n  # }\n}\n</code></pre> <p>Resource type: <code>aws_instance</code></p>"},{"location":"policies/cost/#cost-003","title":"COST-003","text":"<p>Previous Generation Instance | LOW</p> <p>Frameworks: --</p> <p>EC2 instance uses previous generation family (m4, m3, c4, c3, r4, r3, i3, d2, t2). Previous generation instance types offer lower performance per dollar compared to current generation equivalents. Upgrading typically provides better performance at the same or lower cost.</p> <p>Remediation:</p> <pre><code>resource \"aws_instance\" \"example\" {\n  # Upgrade from previous generation to current generation:\n  #   m3/m4  -&gt; m5 or m6i\n  #   c3/c4  -&gt; c5 or c6i\n  #   r3/r4  -&gt; r5 or r6i\n  #   i3     -&gt; i3en or i4i\n  #   d2     -&gt; d3 or d3en\n  #   t2     -&gt; t3 or t3a\n  instance_type = \"m5.xlarge\"  # Upgraded from m4.xlarge\n}\n</code></pre> <p>Resource type: <code>aws_instance</code></p>"},{"location":"policies/ebs/","title":"EBS Policies","text":"<p>2 policies covering storage encryption.</p> ID Name Severity Frameworks EBS-001 EBS Volume Encryption HIGH HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2 EBS-002 EBS Snapshot Encryption HIGH HIPAA, PCI DSS, ISO 27001, GDPR"},{"location":"policies/ebs/#ebs-001","title":"EBS-001","text":"<p>EBS Volume Encryption | HIGH</p> <p>Frameworks: HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2</p> <p>EBS volume must have encryption enabled. Unencrypted EBS volumes expose data at rest to unauthorized access if the underlying storage media is compromised. Enabling encryption ensures that data, snapshots, and disk I/O are all protected using AES-256 encryption.</p> <p>Remediation:</p> <pre><code>resource \"aws_ebs_volume\" \"example\" {\n  availability_zone = \"us-east-1a\"\n  size              = 100\n  encrypted         = true\n}\n</code></pre> <p>Resource type: <code>aws_ebs_volume</code></p>"},{"location":"policies/ebs/#ebs-002","title":"EBS-002","text":"<p>EBS Snapshot Encryption | HIGH</p> <p>Frameworks: HIPAA, PCI DSS, ISO 27001, GDPR</p> <p>EBS snapshot copy must have encryption enabled. When copying snapshots across regions or accounts, encryption must be explicitly enabled on the copy to ensure data remains protected in transit and at rest at the destination.</p> <p>Remediation:</p> <pre><code>resource \"aws_ebs_snapshot_copy\" \"example\" {\n  source_snapshot_id = aws_ebs_snapshot.source.id\n  source_region      = \"us-east-1\"\n  encrypted          = true\n}\n</code></pre> <p>Resource type: <code>aws_ebs_snapshot_copy</code></p>"},{"location":"policies/ec2/","title":"EC2 Policies","text":"<p>4 policies covering EC2 instance metadata, encryption, network exposure, and cost optimization.</p> ID Name Severity Frameworks EC2-001 EC2 IMDSv2 Required MEDIUM PCI DSS, ISO 27001, SOC 2 EC2-002 EC2 Root Volume Encryption HIGH HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2 EC2-003 EC2 Public IP Warning MEDIUM PCI DSS, ISO 27001, SOC 2 EC2-005 EC2 Large Instance Review MEDIUM --"},{"location":"policies/ec2/#ec2-001","title":"EC2-001","text":"<p>EC2 IMDSv2 Required | MEDIUM</p> <p>Frameworks: PCI DSS, ISO 27001, SOC 2</p> <p>EC2 instance should require IMDSv2 (<code>http_tokens = required</code>). IMDSv1 is vulnerable to Server-Side Request Forgery (SSRF) attacks that can expose instance credentials.</p> <p>Remediation:</p> <pre><code>resource \"aws_instance\" \"example\" {\n  ami           = \"ami-0123456789abcdef0\"\n  instance_type = \"t3.micro\"\n\n  metadata_options {\n    http_endpoint = \"enabled\"\n    http_tokens   = \"required\"\n  }\n}\n</code></pre> <p>Resource type: <code>aws_instance</code></p>"},{"location":"policies/ec2/#ec2-002","title":"EC2-002","text":"<p>EC2 Root Volume Encryption | HIGH</p> <p>Frameworks: HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2</p> <p>EC2 instance must have an encrypted root volume. Unencrypted volumes risk exposing sensitive data at rest, violating multiple compliance frameworks.</p> <p>Remediation:</p> <pre><code>resource \"aws_instance\" \"example\" {\n  ami           = \"ami-0123456789abcdef0\"\n  instance_type = \"t3.micro\"\n\n  root_block_device {\n    encrypted  = true\n    kms_key_id = aws_kms_key.example.arn\n  }\n}\n</code></pre> <p>Resource type: <code>aws_instance</code></p>"},{"location":"policies/ec2/#ec2-003","title":"EC2-003","text":"<p>EC2 Public IP Warning | MEDIUM</p> <p>Frameworks: PCI DSS, ISO 27001, SOC 2</p> <p>EC2 instance will have a public IP assigned. Instances with public IPs are directly reachable from the internet, increasing the attack surface. Use a load balancer or NAT gateway for outbound access instead.</p> <p>Remediation:</p> <pre><code>resource \"aws_instance\" \"example\" {\n  ami           = \"ami-0123456789abcdef0\"\n  instance_type = \"t3.micro\"\n\n  associate_public_ip_address = false\n}\n</code></pre> <p>Resource type: <code>aws_instance</code></p>"},{"location":"policies/ec2/#ec2-005","title":"EC2-005","text":"<p>EC2 Large Instance Review | MEDIUM</p> <p>Category: cost</p> <p>Frameworks: --</p> <p>EC2 instance uses a very large or expensive instance type. Large instances significantly increase cloud spend and may be over-provisioned for the workload. Review whether right-sizing or Spot Instances can reduce costs.</p> <p>Remediation:</p> <pre><code>resource \"aws_instance\" \"example\" {\n  # Consider downsizing from large instance types (e.g., x1e.32xlarge)\n  # to a smaller instance that fits your workload requirements\n  instance_type = \"m5.xlarge\"\n\n  # Alternatively, use a Spot Instance for fault-tolerant workloads\n  # instance_market_options {\n  #   market_type = \"spot\"\n  #   spot_options {\n  #     max_price = \"0.05\"\n  #   }\n  # }\n}\n</code></pre> <p>Resource type: <code>aws_instance</code></p>"},{"location":"policies/elb/","title":"ELB Policies","text":"<p>3 policies covering load balancer security.</p> ID Name Severity Frameworks ELB-001 ALB Access Logging MEDIUM HIPAA, PCI DSS, ISO 27001, SOC 2 ELB-002 ALB HTTPS Listener Required HIGH HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2 ELB-003 ALB Deletion Protection MEDIUM ISO 27001, SOC 2"},{"location":"policies/elb/#elb-001","title":"ELB-001","text":"<p>ALB Access Logging | MEDIUM</p> <p>Frameworks: HIPAA, PCI DSS, ISO 27001, SOC 2</p> <p>Application Load Balancer should have access logging enabled. Access logs capture detailed information about requests sent to the load balancer, including client IP, latencies, and server responses, which are essential for security analysis, troubleshooting, and compliance auditing.</p> <p>Remediation:</p> <pre><code>resource \"aws_lb\" \"example\" {\n  name               = \"example-alb\"\n  internal           = false\n  load_balancer_type = \"application\"\n  subnets            = var.subnet_ids\n\n  access_logs {\n    bucket  = aws_s3_bucket.lb_logs.id\n    prefix  = \"alb-logs\"\n    enabled = true\n  }\n}\n</code></pre> <p>Resource type: <code>aws_lb</code></p>"},{"location":"policies/elb/#elb-002","title":"ELB-002","text":"<p>ALB HTTPS Listener Required | HIGH</p> <p>Frameworks: HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2</p> <p>Load balancer listener uses protocol other than HTTPS/TLS. Using unencrypted protocols (HTTP) for load balancer listeners exposes traffic to interception and man-in-the-middle attacks. All listeners should use HTTPS or TLS to ensure data is encrypted in transit.</p> <p>Remediation:</p> <pre><code>resource \"aws_lb_listener\" \"example\" {\n  load_balancer_arn = aws_lb.example.arn\n  port              = 443\n  protocol          = \"HTTPS\"\n  ssl_policy        = \"ELBSecurityPolicy-TLS13-1-2-2021-06\"\n  certificate_arn   = aws_acm_certificate.example.arn\n\n  default_action {\n    type             = \"forward\"\n    target_group_arn = aws_lb_target_group.example.arn\n  }\n}\n</code></pre> <p>Resource type: <code>aws_lb_listener</code></p>"},{"location":"policies/elb/#elb-003","title":"ELB-003","text":"<p>ALB Deletion Protection | MEDIUM</p> <p>Frameworks: ISO 27001, SOC 2</p> <p>Application Load Balancer does not have deletion protection enabled. Deletion protection prevents accidental or unauthorized removal of a load balancer, which could cause service outages and data loss for applications relying on it.</p> <p>Remediation:</p> <pre><code>resource \"aws_lb\" \"example\" {\n  name                       = \"example-alb\"\n  internal                   = false\n  load_balancer_type         = \"application\"\n  subnets                    = var.subnet_ids\n  enable_deletion_protection = true\n}\n</code></pre> <p>Resource type: <code>aws_lb</code></p>"},{"location":"policies/iam/","title":"IAM Policies","text":"<p>Cloudrift includes 3 built-in policies for AWS Identity and Access Management, covering least-privilege enforcement, policy management best practices, and trust relationship controls.</p>"},{"location":"policies/iam/#summary","title":"Summary","text":"ID Policy Name Severity Frameworks IAM-001 No Wildcard IAM Actions CRITICAL HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2 IAM-002 No Inline Policies on Users MEDIUM PCI DSS, ISO 27001, SOC 2 IAM-003 IAM Role Trust Too Broad HIGH PCI DSS, ISO 27001, SOC 2"},{"location":"policies/iam/#iam-001","title":"IAM-001","text":""},{"location":"policies/iam/#no-wildcard-iam-actions","title":"No Wildcard IAM Actions","text":"<p>CRITICAL</p> <p>Frameworks: HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2</p> <p>Resource type: <code>aws_iam_policy</code></p> <p>Description: IAM policy contains a wildcard (<code>*</code>) action with <code>Allow</code> effect. Wildcard actions grant unrestricted permissions across all AWS services and API calls, violating the principle of least privilege. A compromised principal with <code>Action: \"*\"</code> has full administrative access to the AWS account, enabling data exfiltration, resource destruction, and privilege escalation.</p> <p>Remediation:</p> <p>Replace wildcard <code>Action</code> with specific actions following the least-privilege principle. Identify the exact API calls required by the workload and grant only those permissions.</p> <pre><code>resource \"aws_iam_policy\" \"example\" {\n  name        = \"application-policy\"\n  description = \"Least-privilege policy for application workload\"\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Sid    = \"AllowS3ReadAccess\"\n        Effect = \"Allow\"\n        Action = [\n          \"s3:GetObject\",\n          \"s3:ListBucket\"\n        ]\n        Resource = [\n          \"arn:aws:s3:::my-app-bucket\",\n          \"arn:aws:s3:::my-app-bucket/*\"\n        ]\n      },\n      {\n        Sid    = \"AllowDynamoDBAccess\"\n        Effect = \"Allow\"\n        Action = [\n          \"dynamodb:GetItem\",\n          \"dynamodb:PutItem\",\n          \"dynamodb:Query\"\n        ]\n        Resource = \"arn:aws:dynamodb:us-east-1:123456789012:table/my-app-table\"\n      }\n    ]\n  })\n}\n</code></pre>"},{"location":"policies/iam/#iam-002","title":"IAM-002","text":""},{"location":"policies/iam/#no-inline-policies-on-users","title":"No Inline Policies on Users","text":"<p>MEDIUM</p> <p>Frameworks: PCI DSS, ISO 27001, SOC 2</p> <p>Resource type: <code>aws_iam_user_policy</code></p> <p>Description: IAM user policy uses an inline policy. Inline policies are embedded directly in a single IAM user and cannot be reused, versioned, or centrally managed. This makes auditing permissions difficult and increases the risk of policy sprawl. AWS best practice recommends using managed policies attached to groups or roles instead of inline policies on individual users.</p> <p>Remediation:</p> <p>Convert inline policies to managed policies and attach them to IAM groups or roles using <code>aws_iam_user_policy_attachment</code>. Users should inherit permissions through group membership rather than direct policy attachment.</p> <pre><code># Instead of an inline policy:\n#\n# resource \"aws_iam_user_policy\" \"inline\" {\n#   name   = \"user-inline-policy\"\n#   user   = aws_iam_user.example.name\n#   policy = jsonencode({ ... })\n# }\n\n# Use a managed policy attached via group membership:\n\nresource \"aws_iam_policy\" \"app_read\" {\n  name        = \"app-read-policy\"\n  description = \"Read access for application resources\"\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect = \"Allow\"\n        Action = [\n          \"s3:GetObject\",\n          \"s3:ListBucket\"\n        ]\n        Resource = [\n          \"arn:aws:s3:::my-app-bucket\",\n          \"arn:aws:s3:::my-app-bucket/*\"\n        ]\n      }\n    ]\n  })\n}\n\nresource \"aws_iam_group\" \"developers\" {\n  name = \"developers\"\n}\n\nresource \"aws_iam_group_policy_attachment\" \"developers_app_read\" {\n  group      = aws_iam_group.developers.name\n  policy_arn = aws_iam_policy.app_read.arn\n}\n\nresource \"aws_iam_user_group_membership\" \"example\" {\n  user   = aws_iam_user.example.name\n  groups = [aws_iam_group.developers.name]\n}\n</code></pre>"},{"location":"policies/iam/#iam-003","title":"IAM-003","text":""},{"location":"policies/iam/#iam-role-trust-too-broad","title":"IAM Role Trust Too Broad","text":"<p>HIGH</p> <p>Frameworks: PCI DSS, ISO 27001, SOC 2</p> <p>Resource type: <code>aws_iam_role</code></p> <p>Description: IAM role has an overly broad trust policy allowing any principal. A trust policy with <code>Principal: \"*\"</code> or <code>Principal: {\"AWS\": \"*\"}</code> allows any AWS account, user, or service to assume the role. This effectively makes the role's permissions available to the entire internet (any AWS account) and can lead to unauthorized cross-account access and privilege escalation.</p> <p>Remediation:</p> <p>Restrict the <code>Principal</code> in the trust policy to specific AWS accounts, services, or IAM entities that legitimately need to assume the role.</p> <pre><code>resource \"aws_iam_role\" \"example\" {\n  name = \"application-role\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Sid    = \"AllowEC2Assume\"\n        Effect = \"Allow\"\n        Principal = {\n          Service = \"ec2.amazonaws.com\"\n        }\n        Action = \"sts:AssumeRole\"\n      },\n      {\n        Sid    = \"AllowCrossAccountAccess\"\n        Effect = \"Allow\"\n        Principal = {\n          AWS = \"arn:aws:iam::123456789012:root\"\n        }\n        Action = \"sts:AssumeRole\"\n        Condition = {\n          StringEquals = {\n            \"sts:ExternalId\" = \"unique-external-id\"\n          }\n        }\n      }\n    ]\n  })\n}\n</code></pre>"},{"location":"policies/kms/","title":"KMS Policies","text":"<p>2 policies covering key management.</p> ID Name Severity Frameworks KMS-001 KMS Key Rotation Enabled HIGH HIPAA, PCI DSS, ISO 27001, SOC 2 KMS-002 KMS Key Deletion Window MEDIUM ISO 27001, SOC 2"},{"location":"policies/kms/#kms-001","title":"KMS-001","text":"<p>KMS Key Rotation Enabled | HIGH</p> <p>Frameworks: HIPAA, PCI DSS, ISO 27001, SOC 2</p> <p>KMS key must have automatic key rotation enabled. Automatic annual rotation of KMS keys reduces the risk of key compromise by limiting the amount of data encrypted under a single key version and satisfying regulatory requirements for cryptographic key lifecycle management.</p> <p>Remediation:</p> <pre><code>resource \"aws_kms_key\" \"example\" {\n  description         = \"Example KMS key\"\n  enable_key_rotation = true\n}\n</code></pre> <p>Resource type: <code>aws_kms_key</code></p>"},{"location":"policies/kms/#kms-002","title":"KMS-002","text":"<p>KMS Key Deletion Window | MEDIUM</p> <p>Frameworks: ISO 27001, SOC 2</p> <p>KMS key has deletion window less than 14 days. A short deletion window increases the risk of accidental permanent key loss. Setting a minimum of 14 days provides adequate time to detect and cancel unintended key deletions before encrypted data becomes permanently inaccessible.</p> <p>Remediation:</p> <pre><code>resource \"aws_kms_key\" \"example\" {\n  description             = \"Example KMS key\"\n  deletion_window_in_days = 14\n}\n</code></pre> <p>Resource type: <code>aws_kms_key</code></p>"},{"location":"policies/lambda/","title":"Lambda Policies","text":"<p>2 policies covering serverless security.</p> ID Name Severity Frameworks LAMBDA-001 Lambda Tracing Enabled MEDIUM SOC 2, ISO 27001 LAMBDA-002 Lambda VPC Configuration MEDIUM HIPAA, PCI DSS, ISO 27001"},{"location":"policies/lambda/#lambda-001","title":"LAMBDA-001","text":"<p>Lambda Tracing Enabled | MEDIUM</p> <p>Frameworks: SOC 2, ISO 27001</p> <p>Lambda function should have X-Ray tracing enabled for observability. Active tracing helps identify performance bottlenecks, trace requests across distributed services, and provides the audit trail needed for compliance frameworks.</p> <p>Remediation:</p> <pre><code>resource \"aws_lambda_function\" \"example\" {\n  function_name = \"example-function\"\n  role          = aws_iam_role.lambda.arn\n  handler       = \"index.handler\"\n  runtime       = \"nodejs18.x\"\n\n  tracing_config {\n    mode = \"Active\"\n  }\n}\n</code></pre> <p>Resource type: <code>aws_lambda_function</code></p>"},{"location":"policies/lambda/#lambda-002","title":"LAMBDA-002","text":"<p>Lambda VPC Configuration | MEDIUM</p> <p>Frameworks: HIPAA, PCI DSS, ISO 27001</p> <p>Lambda function is not configured to run in a VPC. Running Lambda functions inside a VPC enables access to private resources such as RDS databases and ElastiCache clusters, and allows network-level controls via security groups and NACLs required by compliance frameworks.</p> <p>Remediation:</p> <pre><code>resource \"aws_lambda_function\" \"example\" {\n  function_name = \"example-function\"\n  role          = aws_iam_role.lambda.arn\n  handler       = \"index.handler\"\n  runtime       = \"nodejs18.x\"\n\n  vpc_config {\n    subnet_ids         = [aws_subnet.private_a.id, aws_subnet.private_b.id]\n    security_group_ids = [aws_security_group.lambda.id]\n  }\n}\n</code></pre> <p>Resource type: <code>aws_lambda_function</code></p>"},{"location":"policies/logging/","title":"CloudWatch Logs Policies","text":"<p>2 policies covering log management.</p> ID Name Severity Frameworks LOG-001 CloudWatch Log Group Encryption MEDIUM HIPAA, PCI DSS, GDPR, SOC 2 LOG-002 CloudWatch Log Retention MEDIUM HIPAA, GDPR, SOC 2, ISO 27001"},{"location":"policies/logging/#log-001","title":"LOG-001","text":"<p>CloudWatch Log Group Encryption | MEDIUM</p> <p>Frameworks: HIPAA, PCI DSS, GDPR, SOC 2</p> <p>CloudWatch Log Group should be encrypted with a KMS key. By default, log data is encrypted at rest using AWS-managed keys, but using a customer-managed KMS key provides additional control over access policies, key rotation, and audit logging of key usage.</p> <p>Remediation:</p> <pre><code>resource \"aws_cloudwatch_log_group\" \"example\" {\n  name       = \"/aws/lambda/example\"\n  kms_key_id = aws_kms_key.log_encryption.arn\n}\n</code></pre> <p>Resource type: <code>aws_cloudwatch_log_group</code></p>"},{"location":"policies/logging/#log-002","title":"LOG-002","text":"<p>CloudWatch Log Retention | MEDIUM</p> <p>Frameworks: HIPAA, GDPR, SOC 2, ISO 27001</p> <p>CloudWatch Log Group does not have a retention policy configured or retention is 0. Without a retention policy, logs are retained indefinitely, increasing storage costs and potentially violating data retention regulations that require logs to be purged after a defined period.</p> <p>Remediation:</p> <pre><code>resource \"aws_cloudwatch_log_group\" \"example\" {\n  name              = \"/aws/lambda/example\"\n  retention_in_days = 90\n\n  # Common values: 1, 3, 5, 7, 14, 30, 60, 90, 120, 150,\n  # 180, 365, 400, 545, 731, 1096, 1827, 2192, 2557, 2922, 3653\n}\n</code></pre> <p>Resource type: <code>aws_cloudwatch_log_group</code></p>"},{"location":"policies/overview/","title":"Policy Overview","text":"<p>Cloudrift ships with 49 built-in OPA policies covering security, tagging, and cost optimization across 13 AWS resource types.</p>"},{"location":"policies/overview/#summary","title":"Summary","text":"ID Policy Name Severity Category Frameworks S3-001 S3 Encryption Required HIGH security HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2 S3-002 S3 KMS Encryption Recommended LOW security HIPAA, PCI DSS, SOC 2 S3-003 S3 Block Public ACLs HIGH security HIPAA, GDPR, PCI DSS, ISO 27001, SOC 2 S3-004 S3 Block Public Policy HIGH security HIPAA, GDPR, PCI DSS, ISO 27001, SOC 2 S3-005 S3 Ignore Public ACLs HIGH security HIPAA, GDPR, PCI DSS, ISO 27001, SOC 2 S3-006 S3 Restrict Public Buckets HIGH security HIPAA, GDPR, PCI DSS, ISO 27001, SOC 2 S3-007 S3 No Public Read ACL CRITICAL security HIPAA, GDPR, PCI DSS, ISO 27001, SOC 2 S3-008 S3 No Public Read-Write ACL CRITICAL security HIPAA, GDPR, PCI DSS, ISO 27001, SOC 2 S3-009 S3 Versioning Recommended MEDIUM security ISO 27001, SOC 2 EC2-001 EC2 IMDSv2 Required MEDIUM security PCI DSS, ISO 27001, SOC 2 EC2-002 EC2 Root Volume Encryption HIGH security HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2 EC2-003 EC2 Public IP Warning MEDIUM security PCI DSS, ISO 27001, SOC 2 EC2-005 EC2 Large Instance Review MEDIUM cost \u2014 SG-001 No Unrestricted SSH Access CRITICAL security PCI DSS, ISO 27001, SOC 2 SG-002 No Unrestricted RDP Access CRITICAL security PCI DSS, ISO 27001, SOC 2 SG-003 No Unrestricted All Ports Access CRITICAL security PCI DSS, ISO 27001, SOC 2 SG-004 Database Port Public Exposure HIGH security HIPAA, PCI DSS, ISO 27001, SOC 2 RDS-001 RDS Storage Encryption Required HIGH security HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2 RDS-002 RDS No Public Access CRITICAL security HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2 RDS-003 RDS Backup Retention Period MEDIUM security HIPAA, ISO 27001, SOC 2 RDS-004 RDS Deletion Protection MEDIUM security ISO 27001, SOC 2 RDS-005 RDS Multi-AZ Recommended LOW security HIPAA, ISO 27001, SOC 2 IAM-001 No Wildcard IAM Actions CRITICAL security HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2 IAM-002 No Inline Policies on Users MEDIUM security PCI DSS, ISO 27001, SOC 2 IAM-003 IAM Role Trust Too Broad HIGH security PCI DSS, ISO 27001, SOC 2 CT-001 CloudTrail KMS Encryption HIGH security HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2 CT-002 CloudTrail Log File Validation MEDIUM security PCI DSS, ISO 27001, SOC 2 CT-003 CloudTrail Multi-Region MEDIUM security HIPAA, PCI DSS, ISO 27001, SOC 2 KMS-001 KMS Key Rotation Enabled HIGH security HIPAA, PCI DSS, ISO 27001, SOC 2 KMS-002 KMS Key Deletion Window MEDIUM security ISO 27001, SOC 2 ELB-001 ALB Access Logging MEDIUM security HIPAA, PCI DSS, ISO 27001, SOC 2 ELB-002 ALB HTTPS Listener Required HIGH security HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2 ELB-003 ALB Deletion Protection MEDIUM security ISO 27001, SOC 2 EBS-001 EBS Volume Encryption HIGH security HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2 EBS-002 EBS Snapshot Encryption HIGH security HIPAA, PCI DSS, ISO 27001, GDPR LAMBDA-001 Lambda Tracing Enabled MEDIUM security SOC 2, ISO 27001 LAMBDA-002 Lambda VPC Configuration MEDIUM security HIPAA, PCI DSS, ISO 27001 LOG-001 CloudWatch Log Group Encryption MEDIUM security HIPAA, PCI DSS, GDPR, SOC 2 LOG-002 CloudWatch Log Retention MEDIUM security HIPAA, GDPR, SOC 2, ISO 27001 VPC-001 Default Security Group Restrict All HIGH security PCI DSS, ISO 27001, SOC 2 VPC-002 Subnet No Auto-Assign Public IP MEDIUM security PCI DSS, ISO 27001 SECRET-001 Secrets Manager KMS Encryption MEDIUM security HIPAA, PCI DSS, GDPR, SOC 2 SECRET-002 Secrets Rotation Enabled MEDIUM security PCI DSS, ISO 27001, SOC 2 TAG-001 Environment Tag Required MEDIUM tagging SOC 2 TAG-002 Owner Tag Recommended LOW tagging \u2014 TAG-003 Project Tag Recommended LOW tagging \u2014 TAG-004 Name Tag Recommended LOW tagging \u2014 COST-002 Very Large Instance Size LOW cost \u2014 COST-003 Previous Generation Instance LOW cost \u2014"},{"location":"policies/overview/#severity-distribution","title":"Severity Distribution","text":"Severity Count Description CRITICAL 7 Immediate security risk, must fix HIGH 15 Significant security concern MEDIUM 21 Recommended improvement LOW 6 Best practice advisory"},{"location":"policies/overview/#resource-coverage","title":"Resource Coverage","text":"<p>Policies are evaluated for these Terraform resource types:</p> <p><code>aws_s3_bucket</code> <code>aws_instance</code> <code>aws_security_group</code> <code>aws_db_instance</code> <code>aws_iam_policy</code> <code>aws_iam_role</code> <code>aws_iam_user_policy</code> <code>aws_cloudtrail</code> <code>aws_kms_key</code> <code>aws_lb</code> <code>aws_lb_listener</code> <code>aws_ebs_volume</code> <code>aws_ebs_snapshot_copy</code> <code>aws_lambda_function</code> <code>aws_cloudwatch_log_group</code> <code>aws_default_security_group</code> <code>aws_subnet</code> <code>aws_secretsmanager_secret</code></p>"},{"location":"policies/rds/","title":"RDS Policies","text":"<p>Cloudrift includes 5 built-in policies for Amazon RDS, covering database security, encryption, backup resilience, and high availability.</p>"},{"location":"policies/rds/#summary","title":"Summary","text":"ID Policy Name Severity Frameworks RDS-001 RDS Storage Encryption Required HIGH HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2 RDS-002 RDS No Public Access CRITICAL HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2 RDS-003 RDS Backup Retention Period MEDIUM HIPAA, ISO 27001, SOC 2 RDS-004 RDS Deletion Protection MEDIUM ISO 27001, SOC 2 RDS-005 RDS Multi-AZ Recommended LOW HIPAA, ISO 27001, SOC 2"},{"location":"policies/rds/#rds-001","title":"RDS-001","text":""},{"location":"policies/rds/#rds-storage-encryption-required","title":"RDS Storage Encryption Required","text":"<p>HIGH</p> <p>Frameworks: HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2</p> <p>Resource type: <code>aws_db_instance</code></p> <p>Description: RDS instance must have storage encryption enabled. Unencrypted database storage leaves data at rest vulnerable to unauthorized access if the underlying storage media is compromised. Encryption at rest is a baseline requirement for most compliance frameworks handling sensitive or regulated data.</p> <p>Remediation:</p> <p>Enable storage encryption on the RDS instance. Note that encryption can only be enabled at creation time -- existing unencrypted instances must be migrated by creating an encrypted snapshot and restoring from it.</p> <pre><code>resource \"aws_db_instance\" \"example\" {\n  identifier     = \"my-database\"\n  engine         = \"postgres\"\n  engine_version = \"15.4\"\n  instance_class = \"db.t3.medium\"\n\n  allocated_storage = 20\n  storage_encrypted = true\n  kms_key_id        = aws_kms_key.rds.arn\n\n  db_name  = \"mydb\"\n  username = \"admin\"\n  password = var.db_password\n\n  skip_final_snapshot = false\n}\n</code></pre>"},{"location":"policies/rds/#rds-002","title":"RDS-002","text":""},{"location":"policies/rds/#rds-no-public-access","title":"RDS No Public Access","text":"<p>CRITICAL</p> <p>Frameworks: HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2</p> <p>Resource type: <code>aws_db_instance</code></p> <p>Description: RDS instance must not be publicly accessible. A publicly accessible database has a public IP address and can be reached from the internet, making it a direct target for brute-force attacks, SQL injection, and data exfiltration. Databases should reside in private subnets and be accessible only through application-tier resources or VPN connections.</p> <p>Remediation:</p> <p>Set <code>publicly_accessible</code> to <code>false</code> and place the RDS instance in a private subnet group.</p> <pre><code>resource \"aws_db_instance\" \"example\" {\n  identifier     = \"my-database\"\n  engine         = \"postgres\"\n  engine_version = \"15.4\"\n  instance_class = \"db.t3.medium\"\n\n  allocated_storage    = 20\n  publicly_accessible  = false\n  db_subnet_group_name = aws_db_subnet_group.private.name\n\n  db_name  = \"mydb\"\n  username = \"admin\"\n  password = var.db_password\n\n  skip_final_snapshot = false\n}\n\nresource \"aws_db_subnet_group\" \"private\" {\n  name       = \"private-db-subnet-group\"\n  subnet_ids = aws_subnet.private[*].id\n}\n</code></pre>"},{"location":"policies/rds/#rds-003","title":"RDS-003","text":""},{"location":"policies/rds/#rds-backup-retention-period","title":"RDS Backup Retention Period","text":"<p>MEDIUM</p> <p>Frameworks: HIPAA, ISO 27001, SOC 2</p> <p>Resource type: <code>aws_db_instance</code></p> <p>Description: RDS instance has backup retention less than 7 days. Short backup retention periods increase the risk of data loss and limit the ability to recover from accidental deletions, corruption, or security incidents. A minimum of 7 days provides adequate recovery point coverage for most workloads.</p> <p>Remediation:</p> <p>Set <code>backup_retention_period</code> to 7 or higher to ensure sufficient point-in-time recovery coverage.</p> <pre><code>resource \"aws_db_instance\" \"example\" {\n  identifier     = \"my-database\"\n  engine         = \"postgres\"\n  engine_version = \"15.4\"\n  instance_class = \"db.t3.medium\"\n\n  allocated_storage       = 20\n  backup_retention_period = 7\n  backup_window           = \"03:00-04:00\"\n\n  db_name  = \"mydb\"\n  username = \"admin\"\n  password = var.db_password\n\n  skip_final_snapshot = false\n}\n</code></pre>"},{"location":"policies/rds/#rds-004","title":"RDS-004","text":""},{"location":"policies/rds/#rds-deletion-protection","title":"RDS Deletion Protection","text":"<p>MEDIUM</p> <p>Frameworks: ISO 27001, SOC 2</p> <p>Resource type: <code>aws_db_instance</code></p> <p>Description: RDS instance does not have deletion protection enabled. Without deletion protection, the database can be accidentally deleted through the AWS Console, CLI, or API calls, including through Terraform destroy operations. Enabling deletion protection adds a safeguard that requires the protection to be explicitly disabled before the instance can be deleted.</p> <p>Remediation:</p> <p>Enable deletion protection on the RDS instance.</p> <pre><code>resource \"aws_db_instance\" \"example\" {\n  identifier     = \"my-database\"\n  engine         = \"postgres\"\n  engine_version = \"15.4\"\n  instance_class = \"db.t3.medium\"\n\n  allocated_storage   = 20\n  deletion_protection = true\n\n  db_name  = \"mydb\"\n  username = \"admin\"\n  password = var.db_password\n\n  skip_final_snapshot = false\n}\n</code></pre>"},{"location":"policies/rds/#rds-005","title":"RDS-005","text":""},{"location":"policies/rds/#rds-multi-az-recommended","title":"RDS Multi-AZ Recommended","text":"<p>LOW</p> <p>Frameworks: HIPAA, ISO 27001, SOC 2</p> <p>Resource type: <code>aws_db_instance</code></p> <p>Description: RDS instance is not configured for Multi-AZ deployment. Single-AZ deployments are vulnerable to availability zone outages, which can cause extended downtime. Multi-AZ provides a synchronous standby replica in a different availability zone with automatic failover, improving both availability and durability.</p> <p>Remediation:</p> <p>Enable Multi-AZ deployment for production databases to ensure high availability and automatic failover.</p> <pre><code>resource \"aws_db_instance\" \"example\" {\n  identifier     = \"my-database\"\n  engine         = \"postgres\"\n  engine_version = \"15.4\"\n  instance_class = \"db.t3.medium\"\n\n  allocated_storage = 20\n  multi_az          = true\n\n  db_name  = \"mydb\"\n  username = \"admin\"\n  password = var.db_password\n\n  skip_final_snapshot = false\n}\n</code></pre>"},{"location":"policies/s3/","title":"S3 Policies","text":"<p>9 policies covering S3 bucket encryption, public access, and versioning.</p> ID Name Severity Frameworks S3-001 S3 Encryption Required HIGH HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2 S3-002 S3 KMS Encryption Recommended LOW HIPAA, PCI DSS, SOC 2 S3-003 S3 Block Public ACLs HIGH HIPAA, GDPR, PCI DSS, ISO 27001, SOC 2 S3-004 S3 Block Public Policy HIGH HIPAA, GDPR, PCI DSS, ISO 27001, SOC 2 S3-005 S3 Ignore Public ACLs HIGH HIPAA, GDPR, PCI DSS, ISO 27001, SOC 2 S3-006 S3 Restrict Public Buckets HIGH HIPAA, GDPR, PCI DSS, ISO 27001, SOC 2 S3-007 S3 No Public Read ACL CRITICAL HIPAA, GDPR, PCI DSS, ISO 27001, SOC 2 S3-008 S3 No Public Read-Write ACL CRITICAL HIPAA, GDPR, PCI DSS, ISO 27001, SOC 2 S3-009 S3 Versioning Recommended MEDIUM ISO 27001, SOC 2"},{"location":"policies/s3/#s3-001","title":"S3-001","text":"<p>S3 Encryption Required | HIGH</p> <p>Frameworks: HIPAA, PCI DSS, ISO 27001, GDPR, SOC 2</p> <p>S3 bucket must have server-side encryption enabled (planned or live).</p> <p>Remediation:</p> <pre><code>resource \"aws_s3_bucket_server_side_encryption_configuration\" \"example\" {\n  bucket = aws_s3_bucket.example.id\n\n  rule {\n    apply_server_side_encryption_by_default {\n      sse_algorithm = \"AES256\"\n    }\n  }\n}\n</code></pre> <p>Resource type: <code>aws_s3_bucket</code></p>"},{"location":"policies/s3/#s3-002","title":"S3-002","text":"<p>S3 KMS Encryption Recommended | LOW</p> <p>Frameworks: HIPAA, PCI DSS, SOC 2</p> <p>S3 bucket uses AES256 encryption instead of KMS. KMS encryption provides additional controls such as key rotation, audit logging, and fine-grained access policies.</p> <p>Remediation:</p> <pre><code>resource \"aws_s3_bucket_server_side_encryption_configuration\" \"example\" {\n  bucket = aws_s3_bucket.example.id\n\n  rule {\n    apply_server_side_encryption_by_default {\n      sse_algorithm     = \"aws:kms\"\n      kms_master_key_id = aws_kms_key.example.arn\n    }\n  }\n}\n</code></pre> <p>Resource type: <code>aws_s3_bucket</code></p>"},{"location":"policies/s3/#s3-003","title":"S3-003","text":"<p>S3 Block Public ACLs | HIGH</p> <p>Frameworks: HIPAA, GDPR, PCI DSS, ISO 27001, SOC 2</p> <p>S3 bucket must have <code>block_public_acls</code> enabled to prevent new public ACLs from being applied to the bucket or its objects.</p> <p>Remediation:</p> <pre><code>resource \"aws_s3_bucket_public_access_block\" \"example\" {\n  bucket = aws_s3_bucket.example.id\n\n  block_public_acls       = true\n  block_public_policy     = true\n  ignore_public_acls      = true\n  restrict_public_buckets = true\n}\n</code></pre> <p>Resource type: <code>aws_s3_bucket</code></p>"},{"location":"policies/s3/#s3-004","title":"S3-004","text":"<p>S3 Block Public Policy | HIGH</p> <p>Frameworks: HIPAA, GDPR, PCI DSS, ISO 27001, SOC 2</p> <p>S3 bucket must have <code>block_public_policy</code> enabled to prevent bucket policies that grant public access.</p> <p>Remediation:</p> <pre><code>resource \"aws_s3_bucket_public_access_block\" \"example\" {\n  bucket = aws_s3_bucket.example.id\n\n  block_public_acls       = true\n  block_public_policy     = true\n  ignore_public_acls      = true\n  restrict_public_buckets = true\n}\n</code></pre> <p>Resource type: <code>aws_s3_bucket</code></p>"},{"location":"policies/s3/#s3-005","title":"S3-005","text":"<p>S3 Ignore Public ACLs | HIGH</p> <p>Frameworks: HIPAA, GDPR, PCI DSS, ISO 27001, SOC 2</p> <p>S3 bucket must have <code>ignore_public_acls</code> enabled so that any existing public ACLs on the bucket or its objects are ignored and do not grant public access.</p> <p>Remediation:</p> <pre><code>resource \"aws_s3_bucket_public_access_block\" \"example\" {\n  bucket = aws_s3_bucket.example.id\n\n  block_public_acls       = true\n  block_public_policy     = true\n  ignore_public_acls      = true\n  restrict_public_buckets = true\n}\n</code></pre> <p>Resource type: <code>aws_s3_bucket</code></p>"},{"location":"policies/s3/#s3-006","title":"S3-006","text":"<p>S3 Restrict Public Buckets | HIGH</p> <p>Frameworks: HIPAA, GDPR, PCI DSS, ISO 27001, SOC 2</p> <p>S3 bucket must have <code>restrict_public_buckets</code> enabled to restrict access to buckets that have public policies, limiting access to only AWS services and authorized users within the bucket owner's account.</p> <p>Remediation:</p> <pre><code>resource \"aws_s3_bucket_public_access_block\" \"example\" {\n  bucket = aws_s3_bucket.example.id\n\n  block_public_acls       = true\n  block_public_policy     = true\n  ignore_public_acls      = true\n  restrict_public_buckets = true\n}\n</code></pre> <p>Resource type: <code>aws_s3_bucket</code></p>"},{"location":"policies/s3/#s3-007","title":"S3-007","text":"<p>S3 No Public Read ACL | CRITICAL</p> <p>Frameworks: HIPAA, GDPR, PCI DSS, ISO 27001, SOC 2</p> <p>S3 bucket has <code>public-read</code> ACL which is not allowed. Public read access exposes all objects in the bucket to the internet, risking data leakage and compliance violations.</p> <p>Remediation:</p> <pre><code>resource \"aws_s3_bucket_acl\" \"example\" {\n  bucket = aws_s3_bucket.example.id\n  acl    = \"private\"\n}\n</code></pre> <p>Resource type: <code>aws_s3_bucket</code></p>"},{"location":"policies/s3/#s3-008","title":"S3-008","text":"<p>S3 No Public Read-Write ACL | CRITICAL</p> <p>Frameworks: HIPAA, GDPR, PCI DSS, ISO 27001, SOC 2</p> <p>S3 bucket has <code>public-read-write</code> ACL which is extremely dangerous. This grants anyone on the internet both read and write access, allowing data theft, modification, and injection of malicious content.</p> <p>Remediation:</p> <pre><code>resource \"aws_s3_bucket_acl\" \"example\" {\n  bucket = aws_s3_bucket.example.id\n  acl    = \"private\"\n}\n</code></pre> <p>Resource type: <code>aws_s3_bucket</code></p>"},{"location":"policies/s3/#s3-009","title":"S3-009","text":"<p>S3 Versioning Recommended | MEDIUM</p> <p>Frameworks: ISO 27001, SOC 2</p> <p>S3 bucket does not have versioning enabled. Versioning protects against accidental deletions and overwrites by maintaining a complete history of object changes.</p> <p>Remediation:</p> <pre><code>resource \"aws_s3_bucket_versioning\" \"example\" {\n  bucket = aws_s3_bucket.example.id\n\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n</code></pre> <p>Resource type: <code>aws_s3_bucket</code></p>"},{"location":"policies/secrets/","title":"Secrets Manager Policies","text":"<p>2 policies covering secret management.</p> ID Name Severity Frameworks SECRET-001 Secrets Manager KMS Encryption MEDIUM HIPAA, PCI DSS, GDPR, SOC 2 SECRET-002 Secrets Rotation Enabled MEDIUM PCI DSS, ISO 27001, SOC 2"},{"location":"policies/secrets/#secret-001","title":"SECRET-001","text":"<p>Secrets Manager KMS Encryption | MEDIUM</p> <p>Frameworks: HIPAA, PCI DSS, GDPR, SOC 2</p> <p>Secret should use a customer-managed KMS key. By default, Secrets Manager encrypts secrets with an AWS-managed key. Using a customer-managed KMS key provides fine-grained access control, key rotation policies, and detailed audit logging through CloudTrail.</p> <p>Remediation:</p> <pre><code>resource \"aws_secretsmanager_secret\" \"example\" {\n  name       = \"example-secret\"\n  kms_key_id = aws_kms_key.secrets.arn\n}\n</code></pre> <p>Resource type: <code>aws_secretsmanager_secret</code></p>"},{"location":"policies/secrets/#secret-002","title":"SECRET-002","text":"<p>Secrets Rotation Enabled | MEDIUM</p> <p>Frameworks: PCI DSS, ISO 27001, SOC 2</p> <p>Secret does not have automatic rotation configured. Automatic rotation reduces the risk of compromised credentials by periodically replacing secret values without manual intervention, meeting compliance requirements for credential lifecycle management.</p> <p>Remediation:</p> <pre><code>resource \"aws_secretsmanager_secret\" \"example\" {\n  name = \"example-secret\"\n}\n\nresource \"aws_secretsmanager_secret_rotation\" \"example\" {\n  secret_id           = aws_secretsmanager_secret.example.id\n  rotation_lambda_arn = aws_lambda_function.rotation.arn\n\n  rotation_rules {\n    automatically_after_days = 30\n  }\n}\n</code></pre> <p>Resource type: <code>aws_secretsmanager_secret</code></p>"},{"location":"policies/security-groups/","title":"Security Group Policies","text":"<p>Cloudrift includes 4 built-in policies for AWS Security Groups, covering network access controls to prevent unrestricted inbound access to critical ports and services.</p>"},{"location":"policies/security-groups/#summary","title":"Summary","text":"ID Policy Name Severity Frameworks SG-001 No Unrestricted SSH Access CRITICAL PCI DSS, ISO 27001, SOC 2 SG-002 No Unrestricted RDP Access CRITICAL PCI DSS, ISO 27001, SOC 2 SG-003 No Unrestricted All Ports Access CRITICAL PCI DSS, ISO 27001, SOC 2 SG-004 Database Port Public Exposure HIGH HIPAA, PCI DSS, ISO 27001, SOC 2"},{"location":"policies/security-groups/#sg-001","title":"SG-001","text":""},{"location":"policies/security-groups/#no-unrestricted-ssh-access","title":"No Unrestricted SSH Access","text":"<p>CRITICAL</p> <p>Frameworks: PCI DSS, ISO 27001, SOC 2</p> <p>Resource type: <code>aws_security_group</code></p> <p>Description: Security group allows SSH (port 22) from <code>0.0.0.0/0</code>. Unrestricted SSH access exposes instances to brute-force attacks and unauthorized login attempts from any IP address on the internet. SSH access should be limited to known, trusted IP ranges such as corporate networks, bastion hosts, or VPN endpoints.</p> <p>Remediation:</p> <p>Restrict SSH to specific IP ranges; use a bastion host or VPN for remote access.</p> <pre><code>resource \"aws_security_group\" \"example\" {\n  name        = \"restricted-ssh\"\n  description = \"Allow SSH from trusted IPs only\"\n  vpc_id      = aws_vpc.main.id\n\n  ingress {\n    description = \"SSH from corporate network\"\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"10.0.0.0/8\"]  # Replace with your trusted IP range\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n</code></pre>"},{"location":"policies/security-groups/#sg-002","title":"SG-002","text":""},{"location":"policies/security-groups/#no-unrestricted-rdp-access","title":"No Unrestricted RDP Access","text":"<p>CRITICAL</p> <p>Frameworks: PCI DSS, ISO 27001, SOC 2</p> <p>Resource type: <code>aws_security_group</code></p> <p>Description: Security group allows RDP (port 3389) from <code>0.0.0.0/0</code>. Unrestricted RDP access exposes Windows instances to brute-force attacks, credential stuffing, and exploitation of RDP vulnerabilities from any IP on the internet. RDP should be restricted to specific trusted IP ranges.</p> <p>Remediation:</p> <p>Restrict RDP to specific IPs; use a bastion host or VPN for remote access.</p> <pre><code>resource \"aws_security_group\" \"example\" {\n  name        = \"restricted-rdp\"\n  description = \"Allow RDP from trusted IPs only\"\n  vpc_id      = aws_vpc.main.id\n\n  ingress {\n    description = \"RDP from corporate network\"\n    from_port   = 3389\n    to_port     = 3389\n    protocol    = \"tcp\"\n    cidr_blocks = [\"10.0.0.0/8\"]  # Replace with your trusted IP range\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n</code></pre>"},{"location":"policies/security-groups/#sg-003","title":"SG-003","text":""},{"location":"policies/security-groups/#no-unrestricted-all-ports-access","title":"No Unrestricted All Ports Access","text":"<p>CRITICAL</p> <p>Frameworks: PCI DSS, ISO 27001, SOC 2</p> <p>Resource type: <code>aws_security_group</code></p> <p>Description: Security group allows all ports (0-65535) from <code>0.0.0.0/0</code>. Opening all ports to the internet is the broadest possible exposure and makes every running service on the instance publicly reachable. This violates the principle of least privilege and creates a wide attack surface.</p> <p>Remediation:</p> <p>Define specific ports needed and restrict source CIDR ranges to only trusted networks.</p> <pre><code>resource \"aws_security_group\" \"example\" {\n  name        = \"restricted-access\"\n  description = \"Allow only required ports from trusted sources\"\n  vpc_id      = aws_vpc.main.id\n\n  ingress {\n    description = \"HTTPS from internet\"\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  ingress {\n    description = \"Application port from internal network\"\n    from_port   = 8080\n    to_port     = 8080\n    protocol    = \"tcp\"\n    cidr_blocks = [\"10.0.0.0/8\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n</code></pre>"},{"location":"policies/security-groups/#sg-004","title":"SG-004","text":""},{"location":"policies/security-groups/#database-port-public-exposure","title":"Database Port Public Exposure","text":"<p>HIGH</p> <p>Frameworks: HIPAA, PCI DSS, ISO 27001, SOC 2</p> <p>Resource type: <code>aws_security_group</code></p> <p>Description: Security group exposes a database port to <code>0.0.0.0/0</code>. Affected ports include MySQL (3306), PostgreSQL (5432), MSSQL (1433), Oracle (1521), MongoDB (27017), Redis (6379), and Memcached (11211). Publicly exposing database ports allows attackers to directly target database services with brute-force attacks, SQL injection, or exploitation of known vulnerabilities.</p> <p>Remediation:</p> <p>Place databases in private subnets and restrict security group ingress to application-tier security groups or VPN CIDR ranges. Never expose database ports to the public internet.</p> <pre><code>resource \"aws_security_group\" \"database\" {\n  name        = \"database-sg\"\n  description = \"Allow database access from application tier only\"\n  vpc_id      = aws_vpc.main.id\n\n  ingress {\n    description     = \"PostgreSQL from application tier\"\n    from_port       = 5432\n    to_port         = 5432\n    protocol        = \"tcp\"\n    security_groups = [aws_security_group.application.id]\n  }\n\n  ingress {\n    description     = \"MySQL from application tier\"\n    from_port       = 3306\n    to_port         = 3306\n    protocol        = \"tcp\"\n    security_groups = [aws_security_group.application.id]\n  }\n\n  ingress {\n    description     = \"Redis from application tier\"\n    from_port       = 6379\n    to_port         = 6379\n    protocol        = \"tcp\"\n    security_groups = [aws_security_group.application.id]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n</code></pre>"},{"location":"policies/tagging/","title":"Tagging Policies","text":"<p>4 policies covering resource tagging governance.</p> <p>Note</p> <p>TAG-001 is a deny rule (violation). TAG-002, TAG-003, and TAG-004 are warn rules (warnings).</p> <p>Applicable resource types: <code>aws_s3_bucket</code>, <code>aws_instance</code>, <code>aws_security_group</code>, <code>aws_db_instance</code>, <code>aws_rds_cluster</code>, <code>aws_lambda_function</code>, <code>aws_ecs_cluster</code>, <code>aws_eks_cluster</code>, <code>aws_lb</code>, <code>aws_vpc</code>, <code>aws_ebs_volume</code>, <code>aws_kms_key</code>, <code>aws_cloudtrail</code>, <code>aws_cloudwatch_log_group</code>, <code>aws_secretsmanager_secret</code></p> ID Name Severity Frameworks TAG-001 Environment Tag Required MEDIUM SOC 2 TAG-002 Owner Tag Recommended LOW -- TAG-003 Project Tag Recommended LOW -- TAG-004 Name Tag Recommended LOW --"},{"location":"policies/tagging/#tag-001","title":"TAG-001","text":"<p>Environment Tag Required | MEDIUM</p> <p>Action: deny</p> <p>Frameworks: SOC 2</p> <p>Resource is missing required 'Environment' tag. The Environment tag is essential for distinguishing between development, staging, and production resources, enabling proper access controls and change management processes.</p> <p>Remediation:</p> <pre><code>resource \"aws_instance\" \"example\" {\n  ami           = \"ami-0123456789abcdef0\"\n  instance_type = \"t3.micro\"\n\n  tags = {\n    Environment = \"dev\"  # Valid values: dev, staging, production\n  }\n}\n</code></pre> <p>Applicable resource types: <code>aws_s3_bucket</code>, <code>aws_instance</code>, <code>aws_security_group</code>, <code>aws_db_instance</code>, <code>aws_rds_cluster</code>, <code>aws_lambda_function</code>, <code>aws_ecs_cluster</code>, <code>aws_eks_cluster</code>, <code>aws_lb</code>, <code>aws_vpc</code>, <code>aws_ebs_volume</code>, <code>aws_kms_key</code>, <code>aws_cloudtrail</code>, <code>aws_cloudwatch_log_group</code>, <code>aws_secretsmanager_secret</code></p>"},{"location":"policies/tagging/#tag-002","title":"TAG-002","text":"<p>Owner Tag Recommended | LOW</p> <p>Action: warn</p> <p>Frameworks: --</p> <p>Resource is missing 'Owner' tag for accountability. The Owner tag identifies the team or individual responsible for a resource, enabling faster incident response and clearer cost attribution.</p> <p>Remediation:</p> <pre><code>resource \"aws_instance\" \"example\" {\n  ami           = \"ami-0123456789abcdef0\"\n  instance_type = \"t3.micro\"\n\n  tags = {\n    Owner = \"platform-team\"\n  }\n}\n</code></pre> <p>Applicable resource types: <code>aws_s3_bucket</code>, <code>aws_instance</code>, <code>aws_security_group</code>, <code>aws_db_instance</code>, <code>aws_rds_cluster</code>, <code>aws_lambda_function</code>, <code>aws_ecs_cluster</code>, <code>aws_eks_cluster</code>, <code>aws_lb</code>, <code>aws_vpc</code>, <code>aws_ebs_volume</code>, <code>aws_kms_key</code>, <code>aws_cloudtrail</code>, <code>aws_cloudwatch_log_group</code>, <code>aws_secretsmanager_secret</code></p>"},{"location":"policies/tagging/#tag-003","title":"TAG-003","text":"<p>Project Tag Recommended | LOW</p> <p>Action: warn</p> <p>Frameworks: --</p> <p>Resource is missing 'Project' tag for cost allocation. The Project tag enables grouping resources by project in AWS Cost Explorer and billing reports, providing visibility into per-project cloud spend.</p> <p>Remediation:</p> <pre><code>resource \"aws_instance\" \"example\" {\n  ami           = \"ami-0123456789abcdef0\"\n  instance_type = \"t3.micro\"\n\n  tags = {\n    Project = \"cloudrift\"\n  }\n}\n</code></pre> <p>Applicable resource types: <code>aws_s3_bucket</code>, <code>aws_instance</code>, <code>aws_security_group</code>, <code>aws_db_instance</code>, <code>aws_rds_cluster</code>, <code>aws_lambda_function</code>, <code>aws_ecs_cluster</code>, <code>aws_eks_cluster</code>, <code>aws_lb</code>, <code>aws_vpc</code>, <code>aws_ebs_volume</code>, <code>aws_kms_key</code>, <code>aws_cloudtrail</code>, <code>aws_cloudwatch_log_group</code>, <code>aws_secretsmanager_secret</code></p>"},{"location":"policies/tagging/#tag-004","title":"TAG-004","text":"<p>Name Tag Recommended | LOW</p> <p>Action: warn</p> <p>Frameworks: --</p> <p>Resource is missing 'Name' tag. The Name tag provides a human-readable identifier displayed in the AWS console, making it significantly easier to locate and manage resources across accounts and regions.</p> <p>Remediation:</p> <pre><code>resource \"aws_instance\" \"example\" {\n  ami           = \"ami-0123456789abcdef0\"\n  instance_type = \"t3.micro\"\n\n  tags = {\n    Name = \"web-server-01\"\n  }\n}\n</code></pre> <p>Applicable resource types: <code>aws_s3_bucket</code>, <code>aws_instance</code>, <code>aws_security_group</code>, <code>aws_db_instance</code>, <code>aws_rds_cluster</code>, <code>aws_lambda_function</code>, <code>aws_ecs_cluster</code>, <code>aws_eks_cluster</code>, <code>aws_lb</code>, <code>aws_vpc</code>, <code>aws_ebs_volume</code>, <code>aws_kms_key</code>, <code>aws_cloudtrail</code>, <code>aws_cloudwatch_log_group</code>, <code>aws_secretsmanager_secret</code></p>"},{"location":"policies/vpc/","title":"VPC Policies","text":"<p>2 policies covering network security.</p> ID Name Severity Frameworks VPC-001 Default Security Group Restrict All HIGH PCI DSS, ISO 27001, SOC 2 VPC-002 Subnet No Auto-Assign Public IP MEDIUM PCI DSS, ISO 27001"},{"location":"policies/vpc/#vpc-001","title":"VPC-001","text":"<p>Default Security Group Restrict All | HIGH</p> <p>Frameworks: PCI DSS, ISO 27001, SOC 2</p> <p>Default security group must not have any ingress or egress rules. The default security group is automatically associated with instances that are not assigned a custom security group. Leaving rules on the default security group can inadvertently expose resources to unauthorized traffic.</p> <p>Remediation:</p> <pre><code>resource \"aws_default_security_group\" \"default\" {\n  vpc_id = aws_vpc.example.id\n\n  # Remove all ingress and egress rules from the default security group.\n  # Use custom security groups for all traffic rules instead.\n}\n</code></pre> <p>Resource type: <code>aws_default_security_group</code></p>"},{"location":"policies/vpc/#vpc-002","title":"VPC-002","text":"<p>Subnet No Auto-Assign Public IP | MEDIUM</p> <p>Frameworks: PCI DSS, ISO 27001</p> <p>Subnet automatically assigns public IPs to launched instances. Auto-assigning public IPs increases the attack surface by making instances directly reachable from the internet. Use NAT gateways for outbound internet access from private subnets instead.</p> <p>Remediation:</p> <pre><code>resource \"aws_subnet\" \"example\" {\n  vpc_id                  = aws_vpc.example.id\n  cidr_block              = \"10.0.1.0/24\"\n  availability_zone       = \"us-east-1a\"\n  map_public_ip_on_launch = false\n}\n\n# Use a NAT gateway for outbound internet access\nresource \"aws_nat_gateway\" \"example\" {\n  allocation_id = aws_eip.nat.id\n  subnet_id     = aws_subnet.public.id\n}\n</code></pre> <p>Resource type: <code>aws_subnet</code></p>"}]}